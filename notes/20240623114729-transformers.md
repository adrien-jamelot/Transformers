
# Table of Contents

1.  [Resources](#org225828a)
2.  [Project](#orga155c3e)
3.  [Stages](#orgf362ccc)
    1.  [Preprocessing](#org07f4773)
4.  [Implementation](#org069b26e)
        1.  [Training](#org471ac66)
        2.  [Data](#org672e8b7)
        3.  [Metrics](#org828dbfb)
        4.  [Testing](#org748bea8)
    1.  [Parameters](#org4dc49ca)
        1.  [Model](#orgf6850ca)



<a id="org225828a"></a>

# Resources

Original paper: <https://arxiv.org/pdf/1706.03762>
tds: <https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021>


<a id="orga155c3e"></a>

# Project

[Project](file:///c:/Users/adrie/GraphDataScience/Transformers/)


<a id="orgf362ccc"></a>

# Stages


<a id="org07f4773"></a>

## Preprocessing

The article refers to

-   [3]: [Paper: Massive Exploration of Neural Machine Translation](20240624225431-paper_massive_exploration_of_neural_machine_translation.md)

> Sentences were encoded using byte-pair encoding [3], which has a
> shared source target vocabulary of about 37000 tokens

[Byte Pair Encoding](20240624230506-byte_pair_encoding.md)

-   [38]: Google’s neural machine translation system: Bridging the gap
    between human and machine translation.
    
    > For English-French, we used the significantly larger WMT 2014
    > English-French dataset consisting of 36M sentences and split tokens
    > into a 32000 word-piece vocabulary [38].

> Sentence pairs were batched together by approximate sequence
> length. Each training batch contained a set of sentence pairs
> containing approximately 25000 source tokens and 25000 target tokens

I guess for that I'll have to look at their code, or on other online resources.
It's not even that relevant but good to know what kind of dimensions thier playing with.


<a id="org069b26e"></a>

# Implementation

Fundamentally, I want to specify parameters in a config file, send
that to an API and train.


<a id="org471ac66"></a>

### Training

Optimizer, loss, preprocessing, split/cross&#x2026;


<a id="org672e8b7"></a>

### Data

-   `path to the data`


<a id="org828dbfb"></a>

### Metrics

1.  TODO Check MLFlow

2.  Is Tensorboard still a thing?


<a id="org748bea8"></a>

### TODO Testing

1.  TODO Should return helpful exception if args/config are not good


<a id="org4dc49ca"></a>

## Parameters


<a id="orgf6850ca"></a>

### Model

1.  Input Embedding

    > Similarly to other sequence transduction models, we use learned
    > embeddings to convert the input tokens and output tokens to vectors of
    > dimension $d_{model}$. We also use the usual learned liner
    > transformation and softmax function to convert the decoder output to
    > predicted next-token probabilities.
    
    It also refers to [30]: [Using the Output Embedding to Improve Language Models](20240629113938-using_the_output_embedding_to_improve_language_models.md)
    
    1.  Model

2.  Positional Encoding

    > There are many choices of positional encodings, learned and fixed [9].
    
    1.  Formulae in the paper
    
        \begin{align}
        \label{eq:3}
        PE_{(pos,2i)} &= \mathrm{sin}(\frac{\mathrm{pos}}{1000^{\frac{2i}{d_{model}}}})\\
        PE_{(pos,2i+1)} &= \mathrm{cos}(\frac{\mathrm{pos}}{1000^{\frac{2i}{d_{model}}}})\\
        \end{align}
    
    2.  Dimensions
    
        Input: (C,
        Output: 

3.  Transformer hyper-parameters

    -   \#layers
    
    For the first layer:
    
    -   input dimension
    
    For all layers (including the first)
    
    -   output (value) dimension

4.  Multi-head hyper-parameters

    -   $d_{model}$: dimension of the input
    -   $d_v$: dimension of the projected queries.
    -   $h$: number of `heads` (linear projections of the queries.
    
    In the paper they use $d_k=d_v=\frac{d_{model}}{h}=64$ and $h=8$,
    meaning their batch size is 512.
    
    1.  Expression
    
        \begin{align}
        \label{eq:4}
        \mathrm{head}_i&=\mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V)\\
        \mathrm{MultiHead}(Q,K,V) &= \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W^O
        \end{align}
        
        :ID:       92e33e65-396a-40e4-aeb7-263e6ee3a1f4
        
        -   $QW_i^Q: (s, d_k) = (s, d_{model})$
            -   $Q: (s, d_k) = (s,d_{model})$
            -   $W_i^Q : (d_{model}, d_k)$
        -   $KW_i^K: (s, d_k) = (s,d_{model})$
            -   $K: (s, d_k) = (s,d_{model})$
            -   $W_i^K : (d_{model}, d_k)$
        -   $VW_i^V: (s, d_v) = (s,d_{model})$
            -   $V: (s, d_v) = (s, d_{model})$
            -   $W_i^V: (d_{model}, d_v)$
        
        -   $\mathrm{Multihead}(Q,K,V): (s, h \times d_{model}) \times (h \times d_v, d_{model}) = (s, d_{model})$
            -   $\mathrm{Concat}(\mathrm{head}_1, ..., \matrm{head}_h): (s, h \times d_{model})$
                -   $\mathrm{head}_i: (s, d_{model}) \times (d_{model}, s) \times (s, d_{model}) = (s, d_{model})$
            -   $W^O: (h \times d_v, d_{model})$
    
    2.  MultiHead Details
    
        There are three multi-head attentions.
        
        1.  1. In the Encoder:
        
            Simple self attention. The output of the previous
        
        2.  2. In the Encoder-Decoder:
        
            -   Queries: output of the `decoder`.
            -   Keys and Values: output of the `encoder`.
                
                > This allows every position in the decoder to attend over all positions
                > in the input sequence. This mimics the typical encoder-decoder
                > attention mechanisms in sequence-to-sequence models such as [38,2,9]
                
                where 38 refers to [Paper: Google’s Neural Machine Translation System: Bridging the Gap](20240629105534-paper_google_s_neural_machine_translation_system_bridging_the_gap.md)
                
                I'm not sure how the encoder outputs keys and values.

5.  Layer normalization

    > We employ a residual connection [11] around each of
    > the two sub-layers, followed by layer normalization [1].
    
    Where [1] refers to [Paper: Layer Normalization](20240626184526-paper_layer_normalization.md)

6.  Attention model

    Pick between
    
    1.  Dot-Product Attention
    
        \begin{equation}
        \label{eq:2}
        \mathrm{Attention}(Q,K,V)=\mathrm{softmax}(QK^T)V
        \end{equation}
        
        1.  Scaled Dot-Product Attention (paper's preferred)
        
            \begin{equation}
            \label{eq:1}
            \mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
            \end{equation}
            
            1.  Dimensions of the sdpa
            
                -   Q: (s, d<sub>k</sub>)
                -   K: (s, d<sub>k</sub>)
                -   V: (s, d<sub>v</sub>)
                -   Attention(Q,K,V) : (s, d<sub>k</sub>, d<sub>k</sub>, s, s, d<sub>v</sub>) => (s, d<sub>v</sub>)
    
    2.  TODO Additive attention
    
        > Additive attention computes the compatibility function using a feed-forward network with
        > a single hidden layer.
    
    3.  How to pick?
    
        In practice, pick SDPA unless really good reason not too.
        
        1.  Explanation
        
            1.  `Dot-product attention is much faster and more space-efficient` in
                practice as it can be implemented with highly optimized matrix
                multiplication code.
            2.  `Theoretically, they have the same complexity`
            3.  For small $d_k$'s, performances are equal.
            4.  For high $d_k's$, additive outperfoms unscaled dot-product
                attention, but this is counter-balanced with scaling.


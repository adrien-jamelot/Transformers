{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamadri/Transformers/blob/main/model_utils_and_training_drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49b63e5-0c97-470b-817e-0f82f14dfd8d",
      "metadata": {
        "id": "f49b63e5-0c97-470b-817e-0f82f14dfd8d"
      },
      "source": [
        "# Utils and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a72566f-8da4-40a0-851d-54fe28fb7636",
      "metadata": {
        "id": "6a72566f-8da4-40a0-851d-54fe28fb7636"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bcd3cb-50f6-4a83-9113-15a6f1f2ba28",
      "metadata": {
        "id": "03bcd3cb-50f6-4a83-9113-15a6f1f2ba28"
      },
      "source": [
        "### addAndNorm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df6ea7fd-ec7d-4c37-b5e8-73e663b1ec30",
      "metadata": {
        "id": "df6ea7fd-ec7d-4c37-b5e8-73e663b1ec30"
      },
      "outputs": [],
      "source": [
        "\"\"\"Residual connection.\"\"\"\n",
        "\n",
        "\n",
        "def addAndNorm(x, blockOutput, norm):\n",
        "    \"\"\"Residual connection.\"\"\"\n",
        "    return norm(x + blockOutput)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6176556f-9240-493a-93d9-8f3a0c9b42ef",
      "metadata": {
        "id": "6176556f-9240-493a-93d9-8f3a0c9b42ef"
      },
      "source": [
        "### PositionalEncoding.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d9923029-9788-4ce9-8518-2b21105d3929",
      "metadata": {
        "id": "d9923029-9788-4ce9-8518-2b21105d3929"
      },
      "outputs": [],
      "source": [
        "\"\"\"Positional Encoding.\"\"\"\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "def positionalEncoding(x, dim_model):\n",
        "    \"\"\"Positional Encoding.\"\"\"\n",
        "    def sineOrCosine(i):\n",
        "        \"\"\"sin(alpha+pi/2) = cos(alpha).\"\"\"\n",
        "        return math.pi/2*(i % 2 == 1)\n",
        "    values = [\n",
        "        [sineOrCosine(i) + pos/math.pow(10000, 2*(i//2)/dim_model) for\n",
        "         i in range(dim_model)]\n",
        "        for pos in range(x.shape[-2])  # seq_length\n",
        "    ]\n",
        "    return torch.sin(torch.tensor(values)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d82e56-e9f2-4551-9461-6a16c6824a42",
      "metadata": {
        "id": "39d82e56-e9f2-4551-9461-6a16c6824a42"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WlgZBE6f5jIb",
        "outputId": "88c5e40b-f8e9-4b45-f737-e34b06317d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WlgZBE6f5jIb",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Transformers/data"
      ],
      "metadata": {
        "id": "i9_mRnQ766eo",
        "outputId": "c974f87c-b149-49b2-d99b-16e2ea3af227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i9_mRnQ766eo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiny_shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "199362f1-377e-4955-86c2-f775b520b012",
      "metadata": {
        "id": "199362f1-377e-4955-86c2-f775b520b012"
      },
      "outputs": [],
      "source": [
        "tiny_shakespeare = open('/content/drive/MyDrive/Transformers/data/tiny_shakespeare.txt',\n",
        "            'rb').read().decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1106be76-87e9-405c-a431-8569ef02043a",
      "metadata": {
        "id": "1106be76-87e9-405c-a431-8569ef02043a"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9297b179-d38c-4cb9-a023-4f32a6d26367",
      "metadata": {
        "id": "9297b179-d38c-4cb9-a023-4f32a6d26367",
        "outputId": "21466ed7-fb77-441e-cce1-e8879c2208d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=256, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=-1)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=256, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Main transformer block.\n",
        "\n",
        "    Inputs: - model_parameters: ordered dictionary of key-values describing the\n",
        "    layer parameters of the model:\n",
        "      - dim_model: dimension of the model.\n",
        "      - layers: dictionary of key-values describing specific layers\n",
        "        - <layer_name>: dictionary of parameters for the specific multi-head\n",
        "          layer\n",
        "          - attention: dictionary of parameters for the specific attention\n",
        "            function\n",
        "            - dim_key: dimension of the key and query.\n",
        "            - dim_value: dimension of the value.\n",
        "          - nb_head: number of heads.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize parameters.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = model_parameters[\"dim_model\"]\n",
        "        self.encoder = Encoder(model_parameters[\"encoder\"])\n",
        "        self.decoder = Decoder(model_parameters[\"decoder\"])\n",
        "        self.embedding = Embedding(model_parameters)\n",
        "        self.toProba = nn.Sequential(\n",
        "            nn.Linear(self.dim_model,\n",
        "                      model_parameters[\"vocabulary_size\"]),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, lastOutput):\n",
        "        \"\"\"Apply a step forward.\"\"\"\n",
        "        encoderInput = self.embedding(x) + positionalEncoding(\n",
        "            x, self.dim_model)\n",
        "        decoderInput = self.embedding(lastOutput) + positionalEncoding(\n",
        "            lastOutput, self.dim_model)\n",
        "        encoderInput = self.dropout(encoderInput)\n",
        "        decoderInput = self.dropout(decoderInput)\n",
        "        encoderOutput = self.encoder(encoderInput)\n",
        "        decoderOutput = self.decoder(decoderInput, encoderOutput)\n",
        "        lastOutput = self.toProba(decoderOutput)\n",
        "        return lastOutput\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, encoderConfig):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.nb_layers = encoderConfig[\"nb_layers\"]\n",
        "        self.dim_model = encoderConfig[\"dim_model\"]\n",
        "        self.dim_feedforward = encoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model,\n",
        "                                 elementwise_affine=True, bias=True)\n",
        "        self.multiheads = [MultiHeadAttention(encoderConfig[\"multihead\"],\n",
        "                                              masked=False)\n",
        "                           for i in range(self.nb_layers)]\n",
        "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)]\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(x, self.dropout(self.multiheads[i](x, x,\n",
        "                                                               x)),\n",
        "                            self.norm)\n",
        "            x = addAndNorm(h1, self.dropout(self.feedforwards[i](h1)),\n",
        "                           self.norm)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, decoderConfig):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = decoderConfig[\"dim_model\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
        "        self.dim_feedforward = decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.nb_layers = decoderConfig[\"nb_layers\"]\n",
        "        self.layer = []\n",
        "        self.multiheads1 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
        "                                               masked=True)\n",
        "                            for i in range(self.nb_layers)]\n",
        "        self.multiheads2 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
        "                                               masked=False)\n",
        "                            for i in range(self.nb_layers)]\n",
        "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)]\n",
        "\n",
        "    def forward(self, decoderInput, encoderOutput):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(decoderInput,\n",
        "                            self.multiheads1[i](decoderInput,\n",
        "                                                decoderInput,\n",
        "                                                decoderInput),\n",
        "                            self.norm)\n",
        "            h2 = addAndNorm(h1,\n",
        "                            self.multiheads2[i](h1,\n",
        "                                                encoderOutput,\n",
        "                                                encoderOutput),\n",
        "                            self.norm)\n",
        "            lastOutput = addAndNorm(h2,\n",
        "                                    self.feedforwards[i](h2),\n",
        "                                    self.norm)\n",
        "        return lastOutput\n",
        "\n",
        "\n",
        "class LonelyDecoder(nn.Module):\n",
        "    \"\"\"A lonely decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.decoderConfig = model_parameters[\"decoder\"]\n",
        "        self.dim_model = self.decoderConfig[\"dim_model\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
        "        self.dim_feedforward = self.decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.nb_layers = self.decoderConfig[\"nb_layers\"]\n",
        "        self.embedding = Embedding(model_parameters)\n",
        "        self.multiheads1 = nn.ModuleList([MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
        "                                               masked=True)\n",
        "                            for i in range(self.nb_layers)])\n",
        "        #self.multiheads2 = nn.ModuleList([MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
        "        #                                       masked=False)\n",
        "        #                    for i in range(self.nb_layers)])\n",
        "        self.feedforwards = nn.ModuleList([nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)])\n",
        "        self.toLogit = nn.Linear(self.dim_model,\n",
        "                                 model_parameters[\"vocabulary_size\"])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        x = self.embedding(x) + positionalEncoding(\n",
        "            x, self.dim_model)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(x,\n",
        "                            self.dropout(self.multiheads1[i](x, x, x)),\n",
        "                            self.norm)\n",
        "            #h2 = addAndNorm(h1,\n",
        "            #                self.dropout(self.multiheads2[i](h1, h1, h1)),\n",
        "            #                self.norm)\n",
        "            layerOutput = addAndNorm(h1,\n",
        "                                     self.dropout(self.feedforwards[i](h1)),\n",
        "                                     self.norm)\n",
        "        finalOutput = self.toLogit(layerOutput)\n",
        "        return finalOutput\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, masked=False):\n",
        "        \"\"\"Initialize.\n",
        "\n",
        "        Inputs:\n",
        "        - dim_model: model dimension\n",
        "        - masked: prevents tokens to attend to the following ones.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.masked = masked\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"Forward.\n",
        "\n",
        "        Inputs:\n",
        "        - Q: query\n",
        "        - K: key\n",
        "        - V: value\n",
        "        \"\"\"\n",
        "        matmul_0 = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        scaled = torch.divide(matmul_0, torch.Tensor([self.dim_model]))\n",
        "        if self.masked:\n",
        "            mask = torch.ones(scaled.shape)\n",
        "            mask = mask - torch.tril(mask)*mask\n",
        "            mask = torch.where(mask == 1, float('-inf'), 0)\n",
        "            scaled = scaled + mask\n",
        "        softmaxed = self.softmax(scaled)\n",
        "        sdpa = torch.matmul(softmaxed, V)\n",
        "        return sdpa\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention.\n",
        "\n",
        "    Inputs:\n",
        "    - multi_head_config: dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, multi_head_config, masked=False):\n",
        "        \"\"\"Initialize multi-head.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_key = multi_head_config[\"attention\"][\"dim_key\"]\n",
        "        self.dim_value = multi_head_config[\"attention\"][\"dim_value\"]\n",
        "        self.nb_heads = multi_head_config[\"nb_heads\"]\n",
        "        self.dim_model = self.dim_key * self.nb_heads\n",
        "\n",
        "        self.WQs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_key)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.WKs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_key)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.WVs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_value)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.spda = ScaledDotProductAttention(self.dim_model, masked)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"One step of the multi-head block.\"\"\"\n",
        "        heads = [self.spda(self.WQs[i](Q),\n",
        "                           self.WKs[i](K),\n",
        "                           self.WVs[i](V))\n",
        "                 for i in range(self.nb_heads)]\n",
        "        return torch.cat([head for head in heads], -1)\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    \"\"\"Embedding.\"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize embedding.\"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(model_parameters[\"vocabulary_size\"],\n",
        "                                   model_parameters[\"dim_model\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward step.\"\"\"\n",
        "        return self.embedding(x)\n",
        "\n",
        "model_parameters = {\n",
        "    \"dim_model\": 256,\n",
        "    \"vocabulary_size\": 67,\n",
        "    \"batch_size\": 64,\n",
        "    \"encoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 128,\n",
        "                \"dim_value\": 128\n",
        "                },\n",
        "            \"nb_heads\": 2\n",
        "            },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "            }\n",
        "        },\n",
        "    \"decoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"vocabulary_size\": 67,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 64,\n",
        "                \"dim_value\": 64,\n",
        "            },\n",
        "            \"nb_heads\": 4\n",
        "        },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "        }\n",
        "    }\n",
        "}\n",
        "decoder = LonelyDecoder(model_parameters)\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab4af4e-d6f7-4380-b6dd-bc270c76644d",
      "metadata": {
        "id": "2ab4af4e-d6f7-4380-b6dd-bc270c76644d"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1ae36615-9abe-48c3-851b-e1d88cc4ed79",
      "metadata": {
        "id": "1ae36615-9abe-48c3-851b-e1d88cc4ed79",
        "outputId": "154d2223-9e2e-4d40-831a-d61e805263ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "67 unique characters\n",
            "char2idx\n",
            "{'\\n': 0, ' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '3': 10, ':': 11, ';': 12, '?': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'O': 29, 'P': 30, 'Q': 31, 'R': 32, 'S': 33, 'T': 34, 'U': 35, 'V': 36, 'W': 37, 'X': 38, 'Y': 39, 'Z': 40, 'a': 41, 'b': 42, 'c': 43, 'd': 44, 'e': 45, 'f': 46, 'g': 47, 'h': 48, 'i': 49, 'j': 50, 'k': 51, 'l': 52, 'm': 53, 'n': 54, 'o': 55, 'p': 56, 'q': 57, 'r': 58, 's': 59, 't': 60, 'u': 61, 'v': 62, 'w': 63, 'x': 64, 'y': 65, 'z': 66}\n",
            "idx2char\n",
            "['\\n' ' ' '!' '#' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' '@' 'A' 'B' 'C'\n",
            " 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U'\n",
            " 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm'\n",
            " 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "text_as_int\n",
            "[20 49 58 ... 47  9  0]\n",
            "'First Citizen' ---- characters mapped to int ---- >[20 49 58 59 60  1 17 49 60 49 66 45 54]\n",
            "tensor([20, 49, 58,  ..., 47,  9,  0])\n",
            "There are 123933 chunks of 8 characters available for the\n",
            "network training.\n",
            "one_hot_examples shape: torch.Size([123932, 8, 67])\n",
            "First Ci\n",
            "targets shape: torch.Size([123932, 8])\n",
            "irst Cit\n",
            "one_hot_examples[0].shape: torch.Size([8, 67])\n",
            "targets[0].shape: torch.Size([8])\n",
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=512, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=-1)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=512, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.1532,  0.4577, -0.7640,  ...,  0.6408, -0.1626,  0.1651],\n",
              "         [ 0.3437,  0.5492, -0.6154,  ...,  0.4403,  0.1573,  0.8275],\n",
              "         [ 0.5086,  0.4925, -0.5348,  ..., -0.1011, -0.4964,  0.2324],\n",
              "         ...,\n",
              "         [-0.0185,  0.2282, -1.6808,  ...,  0.1551, -0.6751,  0.5988],\n",
              "         [ 0.0041,  0.7301, -1.3693,  ...,  0.1777, -0.5002,  0.4568],\n",
              "         [ 0.1935, -0.0676, -1.0820,  ...,  0.0388, -0.5048,  0.3791]],\n",
              "\n",
              "        [[ 1.7181,  0.8787, -0.3345,  ...,  0.3341,  0.4758,  0.5261],\n",
              "         [ 0.9315,  0.4030, -0.7516,  ...,  0.5031, -0.2916,  0.4182],\n",
              "         [ 0.2445,  0.0314, -0.5543,  ...,  0.0166, -0.6494,  0.5519],\n",
              "         ...,\n",
              "         [-0.1584,  0.2671, -1.2621,  ..., -0.1479, -0.3506,  0.7022],\n",
              "         [ 0.4705,  0.0717, -1.0611,  ...,  0.0998, -0.6710,  0.6897],\n",
              "         [ 0.3777, -0.1349, -1.2209,  ..., -0.0382, -0.5848,  0.0406]],\n",
              "\n",
              "        [[ 1.4505,  0.6350, -1.1654,  ...,  0.2120,  0.2089,  0.2286],\n",
              "         [ 0.9760,  0.9969, -0.8701,  ...,  0.5179, -0.6862,  0.4500],\n",
              "         [ 0.4720,  0.5853, -0.3454,  ..., -0.0422, -0.5988,  0.6877],\n",
              "         ...,\n",
              "         [ 0.1673,  0.3423, -1.2154,  ..., -0.1223, -1.0203,  0.9209],\n",
              "         [ 0.1540,  0.3708, -1.2077,  ..., -0.0150, -0.9978,  0.9483],\n",
              "         [ 0.4365, -0.1250, -1.2341,  ..., -0.3242, -1.0882,  0.2268]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.3150,  0.9733, -0.5976,  ...,  0.4027, -0.2482,  0.4857],\n",
              "         [ 0.6777,  0.4289, -0.0878,  ...,  0.4757, -0.6226,  0.4199],\n",
              "         [ 0.6872,  0.4731, -0.4917,  ...,  0.1347, -0.8196,  0.8256],\n",
              "         ...,\n",
              "         [ 0.2806,  0.5820, -0.8943,  ...,  0.0784, -0.5845,  0.8395],\n",
              "         [ 0.0897,  0.1291, -1.0907,  ..., -0.0543, -0.6170,  0.5499],\n",
              "         [ 0.6611, -0.1796, -1.5537,  ..., -0.1436, -0.4692,  0.0736]],\n",
              "\n",
              "        [[ 1.0187,  1.0030, -0.9486,  ...,  0.2678, -0.2591,  0.3655],\n",
              "         [ 0.6688,  0.4764, -0.1922,  ...,  0.4804, -0.5207,  0.6684],\n",
              "         [ 0.3174,  0.5221, -0.5659,  ...,  0.4586, -0.7938,  1.2397],\n",
              "         ...,\n",
              "         [ 0.1896,  0.4554, -1.1177,  ..., -0.2317, -1.0069,  0.3518],\n",
              "         [ 0.4069,  0.1249, -1.2753,  ..., -0.0969, -0.5131,  0.6210],\n",
              "         [ 0.5053,  0.5344, -1.3491,  ..., -0.6263, -0.5727,  0.2525]],\n",
              "\n",
              "        [[ 1.0550,  0.6984, -0.8346,  ...,  0.4494,  0.3867,  0.6733],\n",
              "         [ 0.2877,  0.1770, -0.9347,  ...,  0.0051, -0.4731,  0.9092],\n",
              "         [ 0.1209,  0.4182, -0.8386,  ...,  0.4409, -0.7578,  0.4689],\n",
              "         ...,\n",
              "         [-0.1707,  0.3572, -1.5778,  ..., -0.1910, -0.6122,  0.7456],\n",
              "         [ 0.5434,  0.0223, -1.7746,  ...,  0.1171, -0.7161,  0.2804],\n",
              "         [ 0.6346,  0.0914, -1.4230,  ..., -0.1659, -0.7023,  0.3560]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# \"\"\"Training.\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# # import Transformers.model.transformer.Transformer\n",
        "\n",
        "text = tiny_shakespeare\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "print(text[:250])\n",
        "\n",
        "# unique characters in the file\n",
        "vocab = sorted(set(text+\"@\"+\"#\")) # @ will be the initial character\n",
        "                                  # and # the final character. They\n",
        "                                  # are not in the text.\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Lookup tables\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(\"char2idx\")\n",
        "print(char2idx)\n",
        "print(\"idx2char\")\n",
        "print(idx2char)\n",
        "print(\"text_as_int\")\n",
        "print(text_as_int)\n",
        "print ('{} ---- characters mapped to int ---- >{}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n",
        "# Create training examples:\n",
        "seq_length = 8\n",
        "examples_per_epoch = len(text)//(seq_length)\n",
        "\n",
        "int_text_tensor = torch.tensor(text_as_int)\n",
        "chunks = torch.chunk(int_text_tensor, examples_per_epoch, 0)\n",
        "print(int_text_tensor)\n",
        "\n",
        "examples = [chunk[:-1] for chunk in chunks]\n",
        "targets = [chunk[1:] for chunk in chunks]\n",
        "print(f\"\"\"There are {len(examples)} chunks of {seq_length} characters available for the\n",
        "network training.\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "model_parameters = {\n",
        "    \"dim_model\": 512,\n",
        "    \"vocabulary_size\": 67,\n",
        "    \"batch_size\": 64,\n",
        "    \"encoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 128,\n",
        "                \"dim_value\": 128\n",
        "                },\n",
        "            \"nb_heads\": 2\n",
        "            },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "            }\n",
        "        },\n",
        "    \"decoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"vocabulary_size\": 67,\n",
        "        \"dim_model\": 512,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 512,\n",
        "                \"dim_key\": 64,\n",
        "                \"dim_value\": 64,\n",
        "            },\n",
        "            \"nb_heads\": 8\n",
        "        },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 512\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# transformer = Transformer(model_parameters)\n",
        "# x = torch.randn(10, 128)\n",
        "# lastOutput = torch.randn(3, 128)\n",
        "# transformer(x, lastOutput)\n",
        "\n",
        "decoder = LonelyDecoder(model_parameters)\n",
        "x = torch.randn(10, model_parameters[\"vocabulary_size\"])\n",
        "lastOutput = torch.randn(3, model_parameters[\"vocabulary_size\"])\n",
        "#decoder(x)\n",
        "\n",
        "\n",
        "one_hot_examples = F.one_hot(torch.stack(examples[:-1]).long(),\n",
        "                             model_parameters[\"vocabulary_size\"]).float()\n",
        "targets = torch.stack(targets[:-1]).long()\n",
        "\n",
        "print(f\"one_hot_examples shape: {one_hot_examples.shape}\")\n",
        "print(''.join([idx2char[i] for i in torch.max(one_hot_examples[0], dim=1)[1].tolist()]))\n",
        "print(f\"targets shape: {targets.shape}\")\n",
        "print(''.join([idx2char[i] for i in targets[0]]))\n",
        "print(f\"one_hot_examples[0].shape: {one_hot_examples[0].shape}\")\n",
        "print(f\"targets[0].shape: {targets[0].shape}\")\n",
        "#decoder(one_hot_examples[0])\n",
        "one_hot_examples[0]\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#loss = loss_fn(decoder(one_hot_examples[0]), one_hot_targets[0])\n",
        "#loss.backward()\n",
        "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3, betas=(0.9, 0.98))\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class customDataset(Dataset):\n",
        "    def __init__(self, data, target):\n",
        "        self.data  = data\n",
        "        self.target = target\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.target[idx]\n",
        "\n",
        "# dataset = customDataset(data, target)\n",
        "dataset = customDataset(one_hot_examples, targets)\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss =  loss_fn(pred.transpose(1,2), y) # loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(f\"{name}: {param.data}\")\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "print(decoder)\n",
        "decoder(next(iter(train_dataloader))[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loop\")\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss =  loss_fn(pred.transpose(1,2), y) # loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "\n",
        "run_n_epochs(10, train_dataloader, decoder, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "i7mbxlZQ6bTa",
        "outputId": "705506be-26c8-4bc6-e3f4-426f29897fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "i7mbxlZQ6bTa",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loop\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.069202  [   64/123932]\n",
            "Input\n",
            "!\n",
            "My sou\n",
            "Target\n",
            "\n",
            "My soul\n",
            "Prediction\n",
            "\n",
            "\n",
            "y so r\n",
            "loss: 1.992315  [ 6464/123932]\n",
            "Input\n",
            ",\n",
            "And aw\n",
            "Target\n",
            "\n",
            "And awf\n",
            "Prediction\n",
            " And tna\n",
            "loss: 1.925008  [12864/123932]\n",
            "Input\n",
            ":\n",
            "Not ma\n",
            "Target\n",
            "\n",
            "Not mad\n",
            "Prediction\n",
            "\n",
            "Tot ten\n",
            "loss: 2.041757  [19264/123932]\n",
            "Input\n",
            "nna\n",
            "Live\n",
            "Target\n",
            "na\n",
            "Live \n",
            "Prediction\n",
            "dotToke \n",
            "loss: 2.013444  [25664/123932]\n",
            "Input\n",
            ", in hea\n",
            "Target\n",
            " in heav\n",
            "Prediction\n",
            " af te r\n",
            "loss: 1.942425  [32064/123932]\n",
            "Input\n",
            " who sha\n",
            "Target\n",
            "who shal\n",
            "Prediction\n",
            "tierwhal\n",
            "loss: 2.010029  [38464/123932]\n",
            "Input\n",
            "imes to \n",
            "Target\n",
            "mes to c\n",
            "Prediction\n",
            "ne tth t\n",
            "loss: 2.021778  [44864/123932]\n",
            "Input\n",
            "less tha\n",
            "Target\n",
            "ess than\n",
            "Prediction\n",
            "l t thet\n",
            "loss: 2.016076  [51264/123932]\n",
            "Input\n",
            "tent!\n",
            "\n",
            "H\n",
            "Target\n",
            "ent!\n",
            "\n",
            "HA\n",
            "Prediction\n",
            "hr  \n",
            "\n",
            "KE\n",
            "loss: 1.992336  [57664/123932]\n",
            "Input\n",
            "shall be\n",
            "Target\n",
            "hall bea\n",
            "Prediction\n",
            " all te \n",
            "loss: 2.048733  [64064/123932]\n",
            "Input\n",
            "illiam L\n",
            "Target\n",
            "lliam Lo\n",
            "Prediction\n",
            "nl nneto\n",
            "loss: 1.908268  [70464/123932]\n",
            "Input\n",
            " wars in\n",
            "Target\n",
            "wars in \n",
            "Prediction\n",
            "tisdttn \n",
            "loss: 1.956177  [76864/123932]\n",
            "Input\n",
            "d,\n",
            "Save \n",
            "Target\n",
            ",\n",
            "Save t\n",
            "Prediction\n",
            "  Thye t\n",
            "loss: 1.993781  [83264/123932]\n",
            "Input\n",
            "\n",
            "LADY GR\n",
            "Target\n",
            "LADY GRE\n",
            "Prediction\n",
            "\n",
            "ERY BAE\n",
            "loss: 1.922192  [89664/123932]\n",
            "Input\n",
            "\n",
            "ANGELO:\n",
            "Target\n",
            "ANGELO:\n",
            "\n",
            "Prediction\n",
            "\n",
            "nTELO:\n",
            "\n",
            "loss: 1.965691  [96064/123932]\n",
            "Input\n",
            "rom him,\n",
            "Target\n",
            "om him,\n",
            "\n",
            "Prediction\n",
            " m tem  \n",
            "loss: 1.957173  [102464/123932]\n",
            "Input\n",
            "? What's\n",
            "Target\n",
            " What's \n",
            "Prediction\n",
            "\n",
            "what s \n",
            "loss: 1.908591  [108864/123932]\n",
            "Input\n",
            "t have m\n",
            "Target\n",
            " have me\n",
            "Prediction\n",
            "htete te\n",
            "loss: 1.897703  [115264/123932]\n",
            "Input\n",
            "him, by \n",
            "Target\n",
            "im, by n\n",
            "Prediction\n",
            "es  te t\n",
            "loss: 1.920086  [121664/123932]\n",
            "Input\n",
            "Brother,\n",
            "Target\n",
            "rother, \n",
            "Prediction\n",
            "uother  \n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.854807  [   64/123932]\n",
            "Input\n",
            "s.\n",
            "\n",
            "WARW\n",
            "Target\n",
            ".\n",
            "\n",
            "WARWI\n",
            "Prediction\n",
            " \n",
            "\n",
            "CARWI\n",
            "loss: 1.900021  [ 6464/123932]\n",
            "Input\n",
            "many gre\n",
            "Target\n",
            "any gree\n",
            "Prediction\n",
            "en  trea\n",
            "loss: 1.834707  [12864/123932]\n",
            "Input\n",
            "orrow?  \n",
            "Target\n",
            "rrow?  W\n",
            "Prediction\n",
            "u own It\n",
            "loss: 1.887662  [19264/123932]\n",
            "Input\n",
            "n'd with\n",
            "Target\n",
            "'d with \n",
            "Prediction\n",
            "ds tith \n",
            "loss: 1.963015  [25664/123932]\n",
            "Input\n",
            "stabb'd \n",
            "Target\n",
            "tabb'd w\n",
            "Prediction\n",
            "  need t\n",
            "loss: 1.921548  [32064/123932]\n",
            "Input\n",
            "ded,\n",
            "But\n",
            "Target\n",
            "ed,\n",
            "But \n",
            "Prediction\n",
            " r  Tut \n",
            "loss: 1.930080  [38464/123932]\n",
            "Input\n",
            "s are ri\n",
            "Target\n",
            " are ric\n",
            "Prediction\n",
            " t e teg\n",
            "loss: 1.873559  [44864/123932]\n",
            "Input\n",
            "etroth'd\n",
            "Target\n",
            "troth'd \n",
            "Prediction\n",
            "  enhed \n",
            "loss: 1.902932  [51264/123932]\n",
            "Input\n",
            "self Pri\n",
            "Target\n",
            "elf Prin\n",
            "Prediction\n",
            "  f taon\n",
            "loss: 1.899941  [57664/123932]\n",
            "Input\n",
            "ring. St\n",
            "Target\n",
            "ing. Sta\n",
            "Prediction\n",
            " ng \n",
            "Iia\n",
            "loss: 1.892673  [64064/123932]\n",
            "Input\n",
            " I, beli\n",
            "Target\n",
            "I, belie\n",
            "Prediction\n",
            "t  au in\n",
            "loss: 1.933024  [70464/123932]\n",
            "Input\n",
            "en and w\n",
            "Target\n",
            "n and we\n",
            "Prediction\n",
            " ttnd ti\n",
            "loss: 1.908160  [76864/123932]\n",
            "Input\n",
            "e market\n",
            "Target\n",
            " market \n",
            "Prediction\n",
            " tanr   \n",
            "loss: 1.950607  [83264/123932]\n",
            "Input\n",
            "him not;\n",
            "Target\n",
            "im not; \n",
            "Prediction\n",
            "es tot  \n",
            "loss: 1.827765  [89664/123932]\n",
            "Input\n",
            "en sleep\n",
            "Target\n",
            "n sleep;\n",
            "Prediction\n",
            "  tha   \n",
            "loss: 1.918900  [96064/123932]\n",
            "Input\n",
            "in,\n",
            "His \n",
            "Target\n",
            "n,\n",
            "His l\n",
            "Prediction\n",
            "sg Aes t\n",
            "loss: 1.933689  [102464/123932]\n",
            "Input\n",
            "we more \n",
            "Target\n",
            "e more h\n",
            "Prediction\n",
            "ertyne t\n",
            "loss: 1.943896  [108864/123932]\n",
            "Input\n",
            " earth?\n",
            "\n",
            "Target\n",
            "earth?\n",
            "S\n",
            "Prediction\n",
            "tnr h \n",
            "\n",
            "\n",
            "loss: 1.890414  [115264/123932]\n",
            "Input\n",
            "at was s\n",
            "Target\n",
            "t was so\n",
            "Prediction\n",
            "t tis th\n",
            "loss: 1.865392  [121664/123932]\n",
            "Input\n",
            "the city\n",
            "Target\n",
            "he city'\n",
            "Prediction\n",
            "hersoti \n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.875450  [   64/123932]\n",
            "Input\n",
            "and Aves\n",
            "Target\n",
            "nd Aves \n",
            "Prediction\n",
            "nd tne t\n",
            "loss: 1.960513  [ 6464/123932]\n",
            "Input\n",
            "ntague, \n",
            "Target\n",
            "tague, o\n",
            "Prediction\n",
            "  nee  t\n",
            "loss: 1.924633  [12864/123932]\n",
            "Input\n",
            " ear\n",
            "To \n",
            "Target\n",
            "ear\n",
            "To c\n",
            "Prediction\n",
            "tnrtTh s\n",
            "loss: 1.884316  [19264/123932]\n",
            "Input\n",
            "e of unm\n",
            "Target\n",
            " of unme\n",
            "Prediction\n",
            " tf tnta\n",
            "loss: 1.930374  [25664/123932]\n",
            "Input\n",
            "GLOUCEST\n",
            "Target\n",
            "LOUCESTE\n",
            "Prediction\n",
            "oOUSESTE\n",
            "loss: 1.811751  [32064/123932]\n",
            "Input\n",
            ":\n",
            "I shal\n",
            "Target\n",
            "\n",
            "I shall\n",
            "Prediction\n",
            "\n",
            "W whall\n",
            "loss: 1.976278  [38464/123932]\n",
            "Input\n",
            "ight now\n",
            "Target\n",
            "ght now.\n",
            "Prediction\n",
            "nht tot \n",
            "loss: 1.944275  [44864/123932]\n",
            "Input\n",
            " in hell\n",
            "Target\n",
            "in hell.\n",
            "Prediction\n",
            "tn te l \n",
            "loss: 1.856466  [51264/123932]\n",
            "Input\n",
            "ucesters\n",
            "Target\n",
            "cestersh\n",
            "Prediction\n",
            " h   r  \n",
            "loss: 1.890071  [57664/123932]\n",
            "Input\n",
            "or with \n",
            "Target\n",
            "r with a\n",
            "Prediction\n",
            "u tith t\n",
            "loss: 2.017775  [64064/123932]\n",
            "Input\n",
            " senate\n",
            "\n",
            "Target\n",
            "senate\n",
            "T\n",
            "Prediction\n",
            "thetti T\n",
            "loss: 1.813730  [70464/123932]\n",
            "Input\n",
            "stiff? s\n",
            "Target\n",
            "tiff? st\n",
            "Prediction\n",
            "  nee\n",
            "Ih\n",
            "loss: 1.943875  [76864/123932]\n",
            "Input\n",
            " the peo\n",
            "Target\n",
            "the peop\n",
            "Prediction\n",
            "the srap\n",
            "loss: 1.962702  [83264/123932]\n",
            "Input\n",
            "women ar\n",
            "Target\n",
            "omen are\n",
            "Prediction\n",
            "oua tt e\n",
            "loss: 1.820035  [89664/123932]\n",
            "Input\n",
            "ort, as \n",
            "Target\n",
            "rt, as i\n",
            "Prediction\n",
            "u h tn t\n",
            "loss: 1.829660  [96064/123932]\n",
            "Input\n",
            "RUMIO:\n",
            "W\n",
            "Target\n",
            "UMIO:\n",
            "Wh\n",
            "Prediction\n",
            "DCION\n",
            "Ah\n",
            "loss: 1.792935  [102464/123932]\n",
            "Input\n",
            "eve.\n",
            "\n",
            "PO\n",
            "Target\n",
            "ve.\n",
            "\n",
            "POM\n",
            "Prediction\n",
            " e \n",
            "\n",
            "LRM\n",
            "loss: 1.897579  [108864/123932]\n",
            "Input\n",
            "talk the\n",
            "Target\n",
            "alk thei\n",
            "Prediction\n",
            "hnl the \n",
            "loss: 1.812238  [115264/123932]\n",
            "Input\n",
            "des in m\n",
            "Target\n",
            "es in me\n",
            "Prediction\n",
            " rttn ty\n",
            "loss: 1.963339  [121664/123932]\n",
            "Input\n",
            "\n",
            "The new\n",
            "Target\n",
            "The news\n",
            "Prediction\n",
            "\n",
            "he sov \n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.842945  [   64/123932]\n",
            "Input\n",
            " thou re\n",
            "Target\n",
            "thou rem\n",
            "Prediction\n",
            "theu ses\n",
            "loss: 1.833406  [ 6464/123932]\n",
            "Input\n",
            "e to ble\n",
            "Target\n",
            " to bles\n",
            "Prediction\n",
            " th teo \n",
            "loss: 1.815998  [12864/123932]\n",
            "Input\n",
            "ill not \n",
            "Target\n",
            "ll not m\n",
            "Prediction\n",
            "nl tot t\n",
            "loss: 1.870771  [19264/123932]\n",
            "Input\n",
            "might kn\n",
            "Target\n",
            "ight kno\n",
            "Prediction\n",
            "enht tio\n",
            "loss: 1.886922  [25664/123932]\n",
            "Input\n",
            "ugh the \n",
            "Target\n",
            "gh the c\n",
            "Prediction\n",
            " htthe s\n",
            "loss: 1.817844  [32064/123932]\n",
            "Input\n",
            "England;\n",
            "Target\n",
            "ngland;\n",
            "\n",
            "Prediction\n",
            "N  end \n",
            "\n",
            "loss: 1.929808  [38464/123932]\n",
            "Input\n",
            "l Kates,\n",
            "Target\n",
            " Kates, \n",
            "Prediction\n",
            "ltite   \n",
            "loss: 1.917846  [44864/123932]\n",
            "Input\n",
            "it the m\n",
            "Target\n",
            "t the mo\n",
            "Prediction\n",
            "nhthe wa\n",
            "loss: 1.906364  [51264/123932]\n",
            "Input\n",
            "all have\n",
            "Target\n",
            "ll have \n",
            "Prediction\n",
            "tl teve \n",
            "loss: 1.955258  [57664/123932]\n",
            "Input\n",
            "e wind;\n",
            "\n",
            "Target\n",
            " wind;\n",
            "N\n",
            "Prediction\n",
            " titd  A\n",
            "loss: 1.859612  [64064/123932]\n",
            "Input\n",
            " 'tis ag\n",
            "Target\n",
            "'tis age\n",
            "Prediction\n",
            "ttis tna\n",
            "loss: 1.799275  [70464/123932]\n",
            "Input\n",
            " mind of\n",
            "Target\n",
            "mind of \n",
            "Prediction\n",
            "tene tf \n",
            "loss: 1.891791  [76864/123932]\n",
            "Input\n",
            "power is\n",
            "Target\n",
            "ower is \n",
            "Prediction\n",
            "eser tn \n",
            "loss: 1.949373  [83264/123932]\n",
            "Input\n",
            "a?\n",
            "Come \n",
            "Target\n",
            "?\n",
            "Come t\n",
            "Prediction\n",
            "t\n",
            "\n",
            "ome t\n",
            "loss: 1.865922  [89664/123932]\n",
            "Input\n",
            "y guiltl\n",
            "Target\n",
            " guiltle\n",
            "Prediction\n",
            " toele y\n",
            "loss: 1.874381  [96064/123932]\n",
            "Input\n",
            "ht them \n",
            "Target\n",
            "t them s\n",
            "Prediction\n",
            "e the  t\n",
            "loss: 1.782076  [102464/123932]\n",
            "Input\n",
            " better \n",
            "Target\n",
            "better t\n",
            "Prediction\n",
            "te ter t\n",
            "loss: 1.810851  [108864/123932]\n",
            "Input\n",
            "ive stat\n",
            "Target\n",
            "ve state\n",
            "Prediction\n",
            "ne thrte\n",
            "loss: 1.952120  [115264/123932]\n",
            "Input\n",
            "now thy \n",
            "Target\n",
            "ow thy m\n",
            "Prediction\n",
            " t the s\n",
            "loss: 1.943436  [121664/123932]\n",
            "Input\n",
            "h'd; and\n",
            "Target\n",
            "'d; and \n",
            "Prediction\n",
            "ed  and \n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.867895  [   64/123932]\n",
            "Input\n",
            "there go\n",
            "Target\n",
            "here gol\n",
            "Prediction\n",
            "he   too\n",
            "loss: 1.871533  [ 6464/123932]\n",
            "Input\n",
            "o, which\n",
            "Target\n",
            ", which \n",
            "Prediction\n",
            "u ahech \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ee6bcdaeb415>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-ee6bcdaeb415>\u001b[0m in \u001b[0;36mrun_n_epochs\u001b[0;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ee6bcdaeb415>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e75f5974ade1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m         x = self.embedding(x) + positionalEncoding(\n\u001b[1;32m    156\u001b[0m             x, self.dim_model)\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             h1 = addAndNorm(x,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " checkpoint = torch.load(\"model_epoch9.pt\")"
      ],
      "metadata": {
        "id": "GYquG9OhwywG"
      },
      "id": "GYquG9OhwywG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint['model_state_dict'].keys()"
      ],
      "metadata": {
        "id": "Q-5QU4T61EIK",
        "outputId": "17113905-c73c-49bd-e4f2-886dae80ed96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q-5QU4T61EIK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['norm.weight', 'norm.bias', 'embedding.embedding.weight', 'embedding.embedding.bias', 'multiheads1.0.WQs.0.weight', 'multiheads1.0.WQs.0.bias', 'multiheads1.0.WQs.1.weight', 'multiheads1.0.WQs.1.bias', 'multiheads1.0.WQs.2.weight', 'multiheads1.0.WQs.2.bias', 'multiheads1.0.WQs.3.weight', 'multiheads1.0.WQs.3.bias', 'multiheads1.0.WQs.4.weight', 'multiheads1.0.WQs.4.bias', 'multiheads1.0.WQs.5.weight', 'multiheads1.0.WQs.5.bias', 'multiheads1.0.WQs.6.weight', 'multiheads1.0.WQs.6.bias', 'multiheads1.0.WQs.7.weight', 'multiheads1.0.WQs.7.bias', 'multiheads1.0.WKs.0.weight', 'multiheads1.0.WKs.0.bias', 'multiheads1.0.WKs.1.weight', 'multiheads1.0.WKs.1.bias', 'multiheads1.0.WKs.2.weight', 'multiheads1.0.WKs.2.bias', 'multiheads1.0.WKs.3.weight', 'multiheads1.0.WKs.3.bias', 'multiheads1.0.WKs.4.weight', 'multiheads1.0.WKs.4.bias', 'multiheads1.0.WKs.5.weight', 'multiheads1.0.WKs.5.bias', 'multiheads1.0.WKs.6.weight', 'multiheads1.0.WKs.6.bias', 'multiheads1.0.WKs.7.weight', 'multiheads1.0.WKs.7.bias', 'multiheads1.0.WVs.0.weight', 'multiheads1.0.WVs.0.bias', 'multiheads1.0.WVs.1.weight', 'multiheads1.0.WVs.1.bias', 'multiheads1.0.WVs.2.weight', 'multiheads1.0.WVs.2.bias', 'multiheads1.0.WVs.3.weight', 'multiheads1.0.WVs.3.bias', 'multiheads1.0.WVs.4.weight', 'multiheads1.0.WVs.4.bias', 'multiheads1.0.WVs.5.weight', 'multiheads1.0.WVs.5.bias', 'multiheads1.0.WVs.6.weight', 'multiheads1.0.WVs.6.bias', 'multiheads1.0.WVs.7.weight', 'multiheads1.0.WVs.7.bias', 'multiheads2.0.WQs.0.weight', 'multiheads2.0.WQs.0.bias', 'multiheads2.0.WQs.1.weight', 'multiheads2.0.WQs.1.bias', 'multiheads2.0.WQs.2.weight', 'multiheads2.0.WQs.2.bias', 'multiheads2.0.WQs.3.weight', 'multiheads2.0.WQs.3.bias', 'multiheads2.0.WQs.4.weight', 'multiheads2.0.WQs.4.bias', 'multiheads2.0.WQs.5.weight', 'multiheads2.0.WQs.5.bias', 'multiheads2.0.WQs.6.weight', 'multiheads2.0.WQs.6.bias', 'multiheads2.0.WQs.7.weight', 'multiheads2.0.WQs.7.bias', 'multiheads2.0.WKs.0.weight', 'multiheads2.0.WKs.0.bias', 'multiheads2.0.WKs.1.weight', 'multiheads2.0.WKs.1.bias', 'multiheads2.0.WKs.2.weight', 'multiheads2.0.WKs.2.bias', 'multiheads2.0.WKs.3.weight', 'multiheads2.0.WKs.3.bias', 'multiheads2.0.WKs.4.weight', 'multiheads2.0.WKs.4.bias', 'multiheads2.0.WKs.5.weight', 'multiheads2.0.WKs.5.bias', 'multiheads2.0.WKs.6.weight', 'multiheads2.0.WKs.6.bias', 'multiheads2.0.WKs.7.weight', 'multiheads2.0.WKs.7.bias', 'multiheads2.0.WVs.0.weight', 'multiheads2.0.WVs.0.bias', 'multiheads2.0.WVs.1.weight', 'multiheads2.0.WVs.1.bias', 'multiheads2.0.WVs.2.weight', 'multiheads2.0.WVs.2.bias', 'multiheads2.0.WVs.3.weight', 'multiheads2.0.WVs.3.bias', 'multiheads2.0.WVs.4.weight', 'multiheads2.0.WVs.4.bias', 'multiheads2.0.WVs.5.weight', 'multiheads2.0.WVs.5.bias', 'multiheads2.0.WVs.6.weight', 'multiheads2.0.WVs.6.bias', 'multiheads2.0.WVs.7.weight', 'multiheads2.0.WVs.7.bias', 'feedforwards.0.0.weight', 'feedforwards.0.0.bias', 'feedforwards.0.2.weight', 'feedforwards.0.2.bias', 'toLogit.weight', 'toLogit.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = LonelyDecoder(model_parameters)\n",
        "mymodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "myoptimizer = torch.optim.Adam(decoder.parameters(), lr=1e-2, betas=(0.9, 0.98))\n",
        "myoptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "print(f\"Loaded checkpoint at epoch {epoch} with loss {loss}\")\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "print(\"Train loop\")\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "run_n_epochs(1, train_dataloader, mymodel, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "MTk8uLaRw7tA",
        "outputId": "80cd89fe-c29c-4859-bcf5-2e9670af8d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        }
      },
      "id": "MTk8uLaRw7tA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint at epoch 10 with loss 0.08523036539554596\n",
            "Train loop\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.264168  [   64/21870]\n",
            "Input\n",
            " at me, make their pastime at my sorrow:\n",
            "They shou\n",
            "Target\n",
            "at me, make their pastime at my sorrow:\n",
            "They shoul\n",
            "Prediction\n",
            "at me, make their pastime at my soreow:\n",
            "They shous\n",
            "loss: 2.465627  [ 6464/21870]\n",
            "Input\n",
            "t want their remedies.\n",
            "Cousin, I am too young to b\n",
            "Target\n",
            " want their remedies.\n",
            "Cousin, I am too young to be\n",
            "Prediction\n",
            " want their remedies.\n",
            "Cousin, I am too young to be\n",
            "loss: 2.635937  [12864/21870]\n",
            "Input\n",
            "h to me to be at enmity;\n",
            "I hate it, and desire all\n",
            "Target\n",
            " to me to be at enmity;\n",
            "I hate it, and desire all \n",
            "Prediction\n",
            " to me to be at enmity;\n",
            "I hate it, and desire alle\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36mrun_n_epochs\u001b[0;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, t)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "totalString = \"ALEXANDRE:\"\n",
        "print(f\"input:\\n{totalString}\")\n",
        "T=10\n",
        "for w in range(500):\n",
        "  input = [char2idx[c] for c in totalString[-8:]]\n",
        "  one_hot_initString = F.one_hot(torch.tensor(input).long(),\n",
        "                             model_parameters[\"vocabulary_size\"]).float()\n",
        "  # print(decoder(one_hot_initString.squeeze(0))[0][-1].shape)\n",
        "  next_char = torch.distributions.Categorical(probs = nn.Softmax()(decoder(one_hot_initString.squeeze(0))[0][-1])/T).sample()\n",
        "  totalString = totalString + idx2char[next_char]\n",
        "\n",
        "# print(one_hot_initString.shape)\n",
        "print(\"Prediction:\")\n",
        "print(totalString) #''.join([idx2char[i] for i in torch.max(decoder(one_hot_initString.squeeze(0))[0],1)[1].tolist()]))\n",
        "print(\"---------------------------------------------\")"
      ],
      "metadata": {
        "id": "fXi_qqUdnmko",
        "outputId": "93065e53-2556-447e-9d90-c4bc16b79c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fXi_qqUdnmko",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\n",
            "ALEXANDRE:\n",
            "Prediction:\n",
            "ALEXANDRE:\n",
            "Tyrank\n",
            "What fear of thy, beforesa come tyrance I was may.\n",
            "I my very worly\n",
            "What my Grean thou art the day, and some\n",
            "And whith mid of the despectings, you steks here. Their his no med whe she hard: a such thou with toed.\n",
            "\n",
            "KING RICHARD III:\n",
            "Shall him the quirest handelose you prack: to cannour, streace's lanotevers of Engle here arm, we storm I knower fain she deatio. That wake foul count the pepolip:\n",
            "In the lightly! every graciest ben his thesed am a.\n",
            "\n",
            "POLIXETER:\n",
            "The sping the joy his dischose, i\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "decoder.eval()\n",
        "print(''.join([idx2char[i] for i in torch.max(x[30], dim=1)[1].tolist()]))\n",
        "print(''.join([idx2char[i] for i in torch.max(decoder(x[30].squeeze(0))[0],1)[1].tolist()]))"
      ],
      "metadata": {
        "id": "31bcN-bLaQZN"
      },
      "id": "31bcN-bLaQZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "decoder.eval()\n",
        "print(''.join([idx2char[i] for i in torch.max(x[30], dim=1)[1].tolist()]))\n",
        "print(''.join([idx2char[i] for i in torch.max(decoder(x[30].squeeze(0))[0],1)[1].tolist()]))\n",
        "\n",
        "decoder.eval()\n",
        "totalString = \"HECTOR:\"\n",
        "idxString = [char2idx[c] for c in totalString[-8:]]\n",
        "tokenizedString = F.one_hot(torch.tensor(idxString).long(),\n",
        "                             model_parameters[\"vocabulary_size\"]).float()\n",
        "\n",
        "print(tokenizedString.shape)\n",
        "for w in range(25):\n",
        "    #print(decoder(tokenizedString.squeeze(0))[0].shape)\n",
        "    next_char_proba = nn.Softmax(dim=-1)(decoder(tokenizedString.squeeze(0))[:,-1])\n",
        "    next_char = torch.max(decoder(tokenizedString.squeeze(0))[0],1)[1].tolist()[-1]\n",
        "    totalString = totalString + idx2char[next_char]\n",
        "    #print(f\"next_char_proba.shape {next_char_proba.shape}\")\n",
        "    #print(f\"tokenizedString.shape {tokenizedString.shape}\")\n",
        "    tokenizedString = torch.cat([tokenizedString[1:], next_char_proba], dim=0)\n",
        "    # print(tokenizedString)\n",
        "    #print(tokenizedString.shape)\n",
        "print(one_hot_initString.shape)\n",
        "print(\"###\")\n",
        "print(totalString) #''.join([idx2char[i] for i in torch.max(decoder(one_hot_initString.squeeze(0))[0],1)[1].tolist()]))\n",
        "print(\"###\")"
      ],
      "metadata": {
        "id": "zZjp_uFDr2qH",
        "outputId": "c513ae7a-9795-4c85-f499-6603df7c3149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zZjp_uFDr2qH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 8, 67])\n",
            "vers, Va\n",
            "e y  aal\n",
            "torch.Size([7, 67])\n",
            "torch.Size([8, 67])\n",
            "###\n",
            "HECTOR:\n",
            "Tore eeel tenea elet ael\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder)"
      ],
      "metadata": {
        "id": "bzA5ndfgKu3O",
        "outputId": "2f12130e-25cf-446e-a87c-aeb9988df2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bzA5ndfgKu3O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=256, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (multiheads2): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=256, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "x,y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(torch.max(x[0], dim=1)[1].tolist())\n",
        "print(y[0])"
      ],
      "metadata": {
        "id": "9tSdscqGACDC",
        "outputId": "e2db63e9-f08d-4c46-8831-dca34ce4643c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9tSdscqGACDC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 8, 67])\n",
            "torch.Size([64, 8])\n",
            "[20, 49, 58, 59, 60, 1, 17, 49]\n",
            "tensor([49, 58, 59, 60,  1, 17, 49, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a15921-6375-4b45-89ab-63391172ce28",
      "metadata": {
        "id": "f5a15921-6375-4b45-89ab-63391172ce28",
        "outputId": "ce2d8939-861a-46f0-8b26-071127d0886e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 4])\n",
            "tensor([[0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "tensor(0.9048)\n"
          ]
        }
      ],
      "source": [
        "# Example of target with class indices\n",
        "loss = nn.CrossEntropyLoss()\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "input = F.one_hot(target, num_classes=5).float()\n",
        "print(target)\n",
        "print(input)\n",
        "output = loss(input, target)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {5+t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "run_n_epochs(45, train_dataloader, decoder, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "TLq7ZWnHr104",
        "outputId": "98f9b8bb-56a2-44ae-9160-82b58e63a5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "TLq7ZWnHr104",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 4.009966  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XHakevm'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 3.983451  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "qZ mfjj,\n",
            "loss: 3.990031  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "obQQfQzZ\n",
            "loss: 4.036373  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "whWQooX \n",
            "loss: 4.001715  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-gQGakn,\n",
            "loss: 4.010240  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "g,#.TTJy\n",
            "loss: 4.002563  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "o-r.merx\n",
            "loss: 4.007478  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "our\n",
            "Miou\n",
            "loss: 3.987219  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "asbib!bj\n",
            "loss: 4.015161  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XuyxVkuc\n",
            "loss: 4.015061  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "yd,\n",
            "wiad\n",
            "loss: 4.015932  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "T&eqtqzt\n",
            "loss: 3.983913  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "Wha.Wr'\n",
            "loss: 4.002244  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "nd;-n-;P\n",
            "loss: 4.000215  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "';'Pjjzn\n",
            "loss: 4.019834  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "b;exQ;Q\n",
            "loss: 3.983692  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q KavevH\n",
            "loss: 4.007519  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q3ousJ;@\n",
            "loss: 4.042298  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            " t,3I Ih\n",
            "loss: 3.993047  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i3-his$q\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 4.009781  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "'D'JI.f.\n",
            "loss: 4.008904  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "haoj3unc\n",
            "loss: 4.020438  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "rrQYyQro\n",
            "loss: 3.994052  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "33;iin z\n",
            "loss: 4.010096  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jxjQjj-j\n",
            "loss: 3.965109  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "33myx c-\n",
            "loss: 3.986863  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "s'zTXs''\n",
            "loss: 3.987497  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "'Mh''Wj'\n",
            "loss: 3.996432  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "SJvX-@xZ\n",
            "loss: 4.019480  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XQQrg&xn\n",
            "loss: 4.010562  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Bikxq;bn\n",
            "loss: 4.002163  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "d3??tzt-\n",
            "loss: 3.961648  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jxjz;bfJ\n",
            "loss: 3.993537  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ".f'TfofJ\n",
            "loss: 3.964676  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jsXqss:\n",
            "\n",
            "loss: 3.987367  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "!n3al!'J\n",
            "loss: 4.018489  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "bexTQ-?r\n",
            "loss: 3.991456  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "PrH FFrJ\n",
            "loss: 4.018596  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "!wJQzo! \n",
            "loss: 4.004164  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ZxxZdZpr\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 3.998062  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "3KikkQkk\n",
            "loss: 4.010931  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q3h&goug\n",
            "loss: 3.999441  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "uX!uzO:l\n",
            "loss: 3.995297  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ".zzzzzz.\n",
            "loss: 3.964694  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ankakBin\n",
            "loss: 4.009676  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i:wizh3w\n",
            "loss: 3.977067  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ikkk! ki\n",
            "loss: 3.997163  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "zmyZWiZt\n",
            "loss: 3.982253  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "OPLT'OY:\n",
            "loss: 3.984232  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-.gj3-g3\n",
            "loss: 4.019158  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ":&ffoD?n\n",
            "loss: 3.983885  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "J3ul!!Zd\n",
            "loss: 3.981462  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "k?qcc?c.\n",
            "loss: 4.005831  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "wiz!iu?-\n",
            "loss: 3.984880  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "mvev;eqv\n",
            "loss: 3.976781  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "emmk m,k\n",
            "loss: 4.008554  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "bppXpk, \n",
            "loss: 3.977186  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XttXt!?w\n",
            "loss: 3.971331  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "CAARCHC\n",
            "loss: 3.974417  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "v?,X!vz!\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 3.993505  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "j'Poxz3c\n",
            "loss: 3.992613  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Ubjavxav\n",
            "loss: 4.007241  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "thq3itt?\n",
            "loss: 4.004530  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-bb?bzZb\n",
            "loss: 3.995043  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i qsqisi\n",
            "loss: 4.002447  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "I:ICVI\n",
            "I\n",
            "loss: 3.974081  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "3cqck3cl\n",
            "loss: 3.973033  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ET\n",
            "Why,\n",
            "\n",
            "loss: 3.994256  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "qD33Q3ex\n",
            "loss: 3.968574  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Hbbzzz;M\n",
            "loss: 4.008118  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "?cq??x?\n",
            "\n",
            "loss: 3.995346  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "xss;s:Z@\n",
            "loss: 3.988353  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ZqbyqPeq\n",
            "loss: 3.990261  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "pcXpVXWh\n",
            "loss: 3.999664  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "JQv!x$rq\n",
            "loss: 3.981891  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "xvJuofuZ\n",
            "loss: 3.998492  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "j;zbhexK\n",
            "loss: 4.008811  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-zirQZ!W\n",
            "loss: 4.020496  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "l??J??-W\n",
            "loss: 3.970493  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "l3q!avll\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 3.997797  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "GRCDCHAG\n",
            "loss: 3.980371  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "r-ZGufZJ\n",
            "loss: 4.005164  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "OOLXL:qw\n",
            "loss: 3.998323  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ",xQ, c,;\n",
            "loss: 3.976596  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "vvvDUsev\n",
            "loss: 3.989638  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "HxYousyZ\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-291c39be0d3b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-291c39be0d3b>\u001b[0m in \u001b[0;36mrun_n_epochs\u001b[0;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {5+t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-38241210b6fc>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W51VPcfq2V76"
      },
      "id": "W51VPcfq2V76",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamadri/Transformers/blob/main/model_utils_and_training_drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49b63e5-0c97-470b-817e-0f82f14dfd8d",
      "metadata": {
        "id": "f49b63e5-0c97-470b-817e-0f82f14dfd8d"
      },
      "source": [
        "# Utils and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a72566f-8da4-40a0-851d-54fe28fb7636",
      "metadata": {
        "id": "6a72566f-8da4-40a0-851d-54fe28fb7636"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bcd3cb-50f6-4a83-9113-15a6f1f2ba28",
      "metadata": {
        "id": "03bcd3cb-50f6-4a83-9113-15a6f1f2ba28"
      },
      "source": [
        "### addAndNorm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df6ea7fd-ec7d-4c37-b5e8-73e663b1ec30",
      "metadata": {
        "id": "df6ea7fd-ec7d-4c37-b5e8-73e663b1ec30"
      },
      "outputs": [],
      "source": [
        "\"\"\"Residual connection.\"\"\"\n",
        "\n",
        "\n",
        "def addAndNorm(x, blockOutput, norm):\n",
        "    \"\"\"Residual connection.\"\"\"\n",
        "    return norm(x + blockOutput)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6176556f-9240-493a-93d9-8f3a0c9b42ef",
      "metadata": {
        "id": "6176556f-9240-493a-93d9-8f3a0c9b42ef"
      },
      "source": [
        "### PositionalEncoding.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d9923029-9788-4ce9-8518-2b21105d3929",
      "metadata": {
        "id": "d9923029-9788-4ce9-8518-2b21105d3929"
      },
      "outputs": [],
      "source": [
        "\"\"\"Positional Encoding.\"\"\"\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "def positionalEncoding(x, dim_model):\n",
        "    \"\"\"Positional Encoding.\"\"\"\n",
        "    def sineOrCosine(i):\n",
        "        \"\"\"sin(alpha+pi/2) = cos(alpha).\"\"\"\n",
        "        return math.pi/2*(i % 2 == 1)\n",
        "    values = [\n",
        "        [sineOrCosine(i) + pos/math.pow(10000, 2*(i//2)/dim_model) for\n",
        "         i in range(dim_model)]\n",
        "        for pos in range(x.shape[-2])  # seq_length\n",
        "    ]\n",
        "    return torch.sin(torch.tensor(values)).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1106be76-87e9-405c-a431-8569ef02043a",
      "metadata": {
        "id": "1106be76-87e9-405c-a431-8569ef02043a"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9297b179-d38c-4cb9-a023-4f32a6d26367",
      "metadata": {
        "id": "9297b179-d38c-4cb9-a023-4f32a6d26367",
        "outputId": "74e63e91-1c32-4e48-9727-ffdd7dc16155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=256, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0-2): 3 x MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (multiheads2): ModuleList(\n",
            "    (0-2): 3 x MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0-2): 3 x Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=256, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Main transformer block.\n",
        "\n",
        "    Inputs: - model_parameters: ordered dictionary of key-values describing the\n",
        "    layer parameters of the model:\n",
        "      - dim_model: dimension of the model.\n",
        "      - layers: dictionary of key-values describing specific layers\n",
        "        - <layer_name>: dictionary of parameters for the specific multi-head\n",
        "          layer\n",
        "          - attention: dictionary of parameters for the specific attention\n",
        "            function\n",
        "            - dim_key: dimension of the key and query.\n",
        "            - dim_value: dimension of the value.\n",
        "          - nb_head: number of heads.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize parameters.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = model_parameters[\"dim_model\"]\n",
        "        self.encoder = Encoder(model_parameters[\"encoder\"])\n",
        "        self.decoder = Decoder(model_parameters[\"decoder\"])\n",
        "        self.embedding = Embedding(model_parameters)\n",
        "        self.toProba = nn.Sequential(\n",
        "            nn.Linear(self.dim_model,\n",
        "                      model_parameters[\"vocabulary_size\"]),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, lastOutput):\n",
        "        \"\"\"Apply a step forward.\"\"\"\n",
        "        encoderInput = self.embedding(x) + positionalEncoding(\n",
        "            x, self.dim_model)\n",
        "        decoderInput = self.embedding(lastOutput) + positionalEncoding(\n",
        "            lastOutput, self.dim_model)\n",
        "        encoderInput = self.dropout(encoderInput)\n",
        "        decoderInput = self.dropout(decoderInput)\n",
        "        encoderOutput = self.encoder(encoderInput)\n",
        "        decoderOutput = self.decoder(decoderInput, encoderOutput)\n",
        "        lastOutput = self.toProba(decoderOutput)\n",
        "        return lastOutput\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder.\"\"\"\n",
        "\n",
        "    def __init__(self, encoderConfig):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.nb_layers = encoderConfig[\"nb_layers\"]\n",
        "        self.dim_model = encoderConfig[\"dim_model\"]\n",
        "        self.dim_feedforward = encoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model,\n",
        "                                 elementwise_affine=True, bias=True)\n",
        "        self.multiheads = [MultiHeadAttention(encoderConfig[\"multihead\"],\n",
        "                                              masked=False)\n",
        "                           for i in range(self.nb_layers)]\n",
        "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)]\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(x, self.dropout(self.multiheads[i](x, x,\n",
        "                                                               x)),\n",
        "                            self.norm)\n",
        "            x = addAndNorm(h1, self.dropout(self.feedforwards[i](h1)),\n",
        "                           self.norm)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, decoderConfig):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = decoderConfig[\"dim_model\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
        "        self.dim_feedforward = decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.nb_layers = decoderConfig[\"nb_layers\"]\n",
        "        self.layer = []\n",
        "        self.multiheads1 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
        "                                               masked=True)\n",
        "                            for i in range(self.nb_layers)]\n",
        "        self.multiheads2 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
        "                                               masked=False)\n",
        "                            for i in range(self.nb_layers)]\n",
        "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)]\n",
        "\n",
        "    def forward(self, decoderInput, encoderOutput):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(decoderInput,\n",
        "                            self.multiheads1[i](decoderInput,\n",
        "                                                decoderInput,\n",
        "                                                decoderInput),\n",
        "                            self.norm)\n",
        "            h2 = addAndNorm(h1,\n",
        "                            self.multiheads2[i](h1,\n",
        "                                                encoderOutput,\n",
        "                                                encoderOutput),\n",
        "                            self.norm)\n",
        "            lastOutput = addAndNorm(h2,\n",
        "                                    self.feedforwards[i](h2),\n",
        "                                    self.norm)\n",
        "        return lastOutput\n",
        "\n",
        "\n",
        "class LonelyDecoder(nn.Module):\n",
        "    \"\"\"A lonely decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.decoderConfig = model_parameters[\"decoder\"]\n",
        "        self.dim_model = self.decoderConfig[\"dim_model\"]\n",
        "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
        "        self.dim_feedforward = self.decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
        "        self.nb_layers = self.decoderConfig[\"nb_layers\"]\n",
        "        self.embedding = Embedding(model_parameters)\n",
        "        self.multiheads1 = nn.ModuleList([MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
        "                                               masked=False)\n",
        "                            for i in range(self.nb_layers)])\n",
        "        self.multiheads2 = nn.ModuleList([MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
        "                                               masked=False)\n",
        "                            for i in range(self.nb_layers)])\n",
        "        self.feedforwards = nn.ModuleList([nn.Sequential(nn.Linear(self.dim_model,\n",
        "                                                     self.dim_feedforward),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Linear(self.dim_feedforward,\n",
        "                                                     self.dim_model))\n",
        "                             for i in range(self.nb_layers)])\n",
        "        self.toLogit = nn.Linear(self.dim_model,\n",
        "                                 model_parameters[\"vocabulary_size\"])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward.\"\"\"\n",
        "        x = self.embedding(x) + positionalEncoding(\n",
        "            x, self.dim_model)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.nb_layers):\n",
        "            h1 = addAndNorm(x,\n",
        "                            self.dropout(self.multiheads1[i](x, x, x)),\n",
        "                            self.norm)\n",
        "            h2 = addAndNorm(h1,\n",
        "                            self.dropout(self.multiheads2[i](h1, h1, h1)),\n",
        "                            self.norm)\n",
        "            layerOutput = addAndNorm(h2,\n",
        "                                     self.dropout(self.feedforwards[i](h2)),\n",
        "                                     self.norm)\n",
        "        finalOutput = self.toLogit(layerOutput)\n",
        "        return finalOutput\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
        "\n",
        "    def __init__(self, dim_model, masked=False):\n",
        "        \"\"\"Initialize.\n",
        "\n",
        "        Inputs:\n",
        "        - dim_model: model dimension\n",
        "        - masked: prevents tokens to attend to the following ones.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.masked = masked\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"Forward.\n",
        "\n",
        "        Inputs:\n",
        "        - Q: query\n",
        "        - K: key\n",
        "        - V: value\n",
        "        \"\"\"\n",
        "        matmul_0 = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        scaled = torch.divide(matmul_0, torch.Tensor([self.dim_model]))\n",
        "        if self.masked:\n",
        "            mask = torch.ones(scaled.shape)\n",
        "            mask = mask - torch.tril(mask)*mask\n",
        "            mask = torch.where(mask == 1, float('-inf'), 0)\n",
        "            scaled = scaled + mask\n",
        "        softmaxed = self.softmax(scaled)\n",
        "        sdpa = torch.matmul(softmaxed, V)\n",
        "        return sdpa\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention.\n",
        "\n",
        "    Inputs:\n",
        "    - multi_head_config: dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, multi_head_config, masked=False):\n",
        "        \"\"\"Initialize multi-head.\"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_key = multi_head_config[\"attention\"][\"dim_key\"]\n",
        "        self.dim_value = multi_head_config[\"attention\"][\"dim_value\"]\n",
        "        self.nb_heads = multi_head_config[\"nb_heads\"]\n",
        "        self.dim_model = self.dim_key * self.nb_heads\n",
        "\n",
        "        self.WQs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_key)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.WKs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_key)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.WVs = nn.ModuleList([nn.Linear(self.dim_model, self.dim_value)\n",
        "                    for i in range(self.nb_heads)])\n",
        "        self.spda = ScaledDotProductAttention(self.dim_model, masked)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        \"\"\"One step of the multi-head block.\"\"\"\n",
        "        heads = [self.spda(self.WQs[i](Q),\n",
        "                           self.WKs[i](K),\n",
        "                           self.WVs[i](V))\n",
        "                 for i in range(self.nb_heads)]\n",
        "        return torch.cat([head for head in heads], -1)\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    \"\"\"Embedding.\"\"\"\n",
        "\n",
        "    def __init__(self, model_parameters):\n",
        "        \"\"\"Initialize embedding.\"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(model_parameters[\"vocabulary_size\"],\n",
        "                                   model_parameters[\"dim_model\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward step.\"\"\"\n",
        "        return self.embedding(x)\n",
        "\n",
        "model_parameters = {\n",
        "    \"dim_model\": 256,\n",
        "    \"vocabulary_size\": 67,\n",
        "    \"batch_size\": 64,\n",
        "    \"encoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 128,\n",
        "                \"dim_value\": 128\n",
        "                },\n",
        "            \"nb_heads\": 2\n",
        "            },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "            }\n",
        "        },\n",
        "    \"decoder\": {\n",
        "        \"nb_layers\": 3,\n",
        "        \"vocabulary_size\": 67,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 32,\n",
        "                \"dim_value\": 32,\n",
        "            },\n",
        "            \"nb_heads\": 8\n",
        "        },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "        }\n",
        "    }\n",
        "}\n",
        "decoder = LonelyDecoder(model_parameters)\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d82e56-e9f2-4551-9461-6a16c6824a42",
      "metadata": {
        "id": "39d82e56-e9f2-4551-9461-6a16c6824a42"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WlgZBE6f5jIb",
        "outputId": "55b2d467-59bc-4646-b2ca-61be2dc15af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WlgZBE6f5jIb",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Transformers/data"
      ],
      "metadata": {
        "id": "i9_mRnQ766eo",
        "outputId": "5f0e1937-7fd9-4347-ef41-07e8c91c8ad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i9_mRnQ766eo",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiny_shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "199362f1-377e-4955-86c2-f775b520b012",
      "metadata": {
        "id": "199362f1-377e-4955-86c2-f775b520b012"
      },
      "outputs": [],
      "source": [
        "tiny_shakespeare = open('/content/drive/MyDrive/Transformers/data/tiny_shakespeare.txt',\n",
        "            'rb').read().decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab4af4e-d6f7-4380-b6dd-bc270c76644d",
      "metadata": {
        "id": "2ab4af4e-d6f7-4380-b6dd-bc270c76644d"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1ae36615-9abe-48c3-851b-e1d88cc4ed79",
      "metadata": {
        "id": "1ae36615-9abe-48c3-851b-e1d88cc4ed79",
        "outputId": "2936c252-99e2-4891-fef5-f2f128aad5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "67 unique characters\n",
            "char2idx\n",
            "{'\\n': 0, ' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '3': 10, ':': 11, ';': 12, '?': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'O': 29, 'P': 30, 'Q': 31, 'R': 32, 'S': 33, 'T': 34, 'U': 35, 'V': 36, 'W': 37, 'X': 38, 'Y': 39, 'Z': 40, 'a': 41, 'b': 42, 'c': 43, 'd': 44, 'e': 45, 'f': 46, 'g': 47, 'h': 48, 'i': 49, 'j': 50, 'k': 51, 'l': 52, 'm': 53, 'n': 54, 'o': 55, 'p': 56, 'q': 57, 'r': 58, 's': 59, 't': 60, 'u': 61, 'v': 62, 'w': 63, 'x': 64, 'y': 65, 'z': 66}\n",
            "idx2char\n",
            "['\\n' ' ' '!' '#' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' '@' 'A' 'B' 'C'\n",
            " 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U'\n",
            " 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm'\n",
            " 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "text_as_int\n",
            "[20 49 58 ... 47  9  0]\n",
            "'First Citizen' ---- characters mapped to int ---- >[20 49 58 59 60  1 17 49 60 49 66 45 54]\n",
            "tensor([20, 49, 58,  ..., 47,  9,  0])\n",
            "There are 21871 chunks of 50 characters available for the\n",
            "network training.\n",
            "one_hot_examples shape: torch.Size([21870, 50, 67])\n",
            "First Citizen:\n",
            "Before we proceed any further, hear\n",
            "targets shape: torch.Size([21870, 50])\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear \n",
            "one_hot_examples[0].shape: torch.Size([50, 67])\n",
            "targets[0].shape: torch.Size([50])\n",
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=256, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (multiheads2): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=256, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"Training.\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# # import Transformers.model.transformer.Transformer\n",
        "\n",
        "text = tiny_shakespeare\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "print(text[:250])\n",
        "\n",
        "# unique characters in the file\n",
        "vocab = sorted(set(text+\"@\"+\"#\")) # @ will be the initial character\n",
        "                                  # and # the final character. They\n",
        "                                  # are not in the text.\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Lookup tables\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(\"char2idx\")\n",
        "print(char2idx)\n",
        "print(\"idx2char\")\n",
        "print(idx2char)\n",
        "print(\"text_as_int\")\n",
        "print(text_as_int)\n",
        "print ('{} ---- characters mapped to int ---- >{}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n",
        "# Create training examples:\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(text)//(seq_length)\n",
        "\n",
        "int_text_tensor = torch.tensor(text_as_int)\n",
        "chunks = torch.chunk(int_text_tensor, examples_per_epoch, 0)\n",
        "print(int_text_tensor)\n",
        "\n",
        "examples = [chunk[:-1] for chunk in chunks]\n",
        "targets = [chunk[1:] for chunk in chunks]\n",
        "print(f\"\"\"There are {len(examples)} chunks of {seq_length} characters available for the\n",
        "network training.\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "model_parameters = {\n",
        "    \"dim_model\": 256,\n",
        "    \"vocabulary_size\": 67,\n",
        "    \"batch_size\": 64,\n",
        "    \"encoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 128,\n",
        "                \"dim_value\": 128\n",
        "                },\n",
        "            \"nb_heads\": 2\n",
        "            },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "            }\n",
        "        },\n",
        "    \"decoder\": {\n",
        "        \"nb_layers\": 1,\n",
        "        \"vocabulary_size\": 67,\n",
        "        \"dim_model\": 256,\n",
        "        \"multihead\": {\n",
        "            \"attention\": {\n",
        "                \"dim_model\": 256,\n",
        "                \"dim_key\": 32,\n",
        "                \"dim_value\": 32,\n",
        "            },\n",
        "            \"nb_heads\": 8\n",
        "        },\n",
        "        \"feedforward\": {\n",
        "            \"dim_feedforward\": 256\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# transformer = Transformer(model_parameters)\n",
        "# x = torch.randn(10, 128)\n",
        "# lastOutput = torch.randn(3, 128)\n",
        "# transformer(x, lastOutput)\n",
        "\n",
        "decoder = LonelyDecoder(model_parameters)\n",
        "x = torch.randn(10, model_parameters[\"vocabulary_size\"])\n",
        "lastOutput = torch.randn(3, model_parameters[\"vocabulary_size\"])\n",
        "#decoder(x)\n",
        "\n",
        "\n",
        "one_hot_examples = F.one_hot(torch.stack(examples[:-1]).long(),\n",
        "                             model_parameters[\"vocabulary_size\"]).float()\n",
        "targets = torch.stack(targets[:-1]).long()\n",
        "\n",
        "print(f\"one_hot_examples shape: {one_hot_examples.shape}\")\n",
        "print(''.join([idx2char[i] for i in torch.max(one_hot_examples[0], dim=1)[1].tolist()]))\n",
        "print(f\"targets shape: {targets.shape}\")\n",
        "print(''.join([idx2char[i] for i in targets[0]]))\n",
        "print(f\"one_hot_examples[0].shape: {one_hot_examples[0].shape}\")\n",
        "print(f\"targets[0].shape: {targets[0].shape}\")\n",
        "#decoder(one_hot_examples[0])\n",
        "one_hot_examples[0]\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#loss = loss_fn(decoder(one_hot_examples[0]), one_hot_targets[0])\n",
        "#loss.backward()\n",
        "optimizer = torch.optim.Adam(decoder.parameters(), lr=5e-3, betas=(0.9, 0.98))\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class customDataset(Dataset):\n",
        "    def __init__(self, data, target):\n",
        "        self.data  = data\n",
        "        self.target = target\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.target[idx]\n",
        "\n",
        "# dataset = customDataset(data, target)\n",
        "dataset = customDataset(one_hot_examples, targets)\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss =  loss_fn(pred.transpose(1,2), y) # loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(f\"{name}: {param.data}\")\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "print(decoder)\n",
        "\n",
        "# device = (\n",
        "#     \"cuda\"\n",
        "#     if torch.cuda.is_available()\n",
        "#     else \"mps\"\n",
        "#     if torch.backends.mps.is_available()\n",
        "#     else \"cpu\"\n",
        "# )\n",
        "# print(f\"Using {device} device\")\n",
        "# #print(transformer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loop\")\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss =  loss_fn(pred.transpose(1,2), y) # loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "\n",
        "run_n_epochs(10, train_dataloader, decoder, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "i7mbxlZQ6bTa",
        "outputId": "03eeb44d-f9a9-4bd7-ffc8-025ffeb2a632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i7mbxlZQ6bTa",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loop\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.313857  [   64/21870]\n",
            "Input\n",
            "\n",
            "Of civil wounds plough'd up with neighbours' swor\n",
            "Target\n",
            "Of civil wounds plough'd up with neighbours' sword\n",
            "Prediction\n",
            "i;;PP$P$O#OOOOX#P#######P#P###OdOOOXOOOOOO\n",
            "\n",
            "\n",
            "OOOOO\n",
            "loss: 2.610475  [ 6464/21870]\n",
            "Input\n",
            "I pinch'd you, Signior Gremio?\n",
            "\n",
            "GREMIO:\n",
            "Two thousa\n",
            "Target\n",
            " pinch'd you, Signior Gremio?\n",
            "\n",
            "GREMIO:\n",
            "Two thousan\n",
            "Prediction\n",
            " ten oe  t  r t:n  nr t   en \n",
            "\n",
            "\n",
            " INENO\n",
            "\n",
            "Ieuthe   n\n",
            "loss: 1.785987  [12864/21870]\n",
            "Input\n",
            ".\n",
            "\n",
            "First Lord:\n",
            "Peace, both, and hear me speak.\n",
            "\n",
            "CO\n",
            "Target\n",
            "\n",
            "\n",
            "First Lord:\n",
            "Peace, both, and hear me speak.\n",
            "\n",
            "COR\n",
            "Prediction\n",
            "\n",
            "\n",
            "\n",
            "init oIre, Meare, touh, tr, tear t  ahear,\n",
            "\n",
            "SoR\n",
            "loss: 1.063768  [19264/21870]\n",
            "Input\n",
            "wn:\n",
            "This cannot be but a great courtier.\n",
            "\n",
            "Shepherd\n",
            "Target\n",
            "n:\n",
            "This cannot be but a great courtier.\n",
            "\n",
            "Shepherd:\n",
            "Prediction\n",
            "n:\n",
            "Whin tannlt he tut t treat hourtinr.\n",
            "\n",
            "Whscverd \n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.796760  [   64/21870]\n",
            "Input\n",
            " watch withal; for, but I be deceived,\n",
            "Our fine mu\n",
            "Target\n",
            "watch withal; for, but I be deceived,\n",
            "Our fine mus\n",
            "Prediction\n",
            "tatoh tinhal; tor, tut t te te eined; Aur tine tur\n",
            "loss: 0.762916  [ 6464/21870]\n",
            "Input\n",
            ",\n",
            "Become them with one half so good a grace\n",
            "As mer\n",
            "Target\n",
            "\n",
            "Become them with one half so good a grace\n",
            "As merc\n",
            "Prediction\n",
            "\n",
            "Become them tith tne talf te tood y Ihace\n",
            "As ter \n",
            "loss: 0.545912  [12864/21870]\n",
            "Input\n",
            "u earth, thou! speak.\n",
            "\n",
            "CALIBAN:\n",
            "\n",
            "PROSPERO:\n",
            "Come fo\n",
            "Target\n",
            " earth, thou! speak.\n",
            "\n",
            "CALIBAN:\n",
            "\n",
            "PROSPERO:\n",
            "Come for\n",
            "Prediction\n",
            " barth, yhou!\n",
            "cpeak.\n",
            "\n",
            "CALOoAN:\n",
            "\n",
            "BROSPERO:\n",
            "Come co \n",
            "loss: 0.201514  [19264/21870]\n",
            "Input\n",
            "AULINA:\n",
            "I am sorry for't:\n",
            "All faults I make, when \n",
            "Target\n",
            "ULINA:\n",
            "I am sorry for't:\n",
            "All faults I make, when I\n",
            "Prediction\n",
            "ULINA:\n",
            "I mm sorry for't:\n",
            "All faults I make, when c\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.156839  [   64/21870]\n",
            "Input\n",
            "sy humour, for no pulse\n",
            "Shall keep his native prog\n",
            "Target\n",
            "y humour, for no pulse\n",
            "Shall keep his native progr\n",
            "Prediction\n",
            "y humour, for oo pulse\n",
            "Shall weep his native progo\n",
            "loss: 0.110296  [ 6464/21870]\n",
            "Input\n",
            "\n",
            "PARIS:\n",
            "Have I thought long to see this morning's \n",
            "Target\n",
            "PARIS:\n",
            "Have I thought long to see this morning's f\n",
            "Prediction\n",
            "PARIS:\n",
            "Have y thought long to see this morning's t\n",
            "loss: 0.193925  [12864/21870]\n",
            "Input\n",
            "ouds are bent\n",
            "To dim his glory and to stain the tr\n",
            "Target\n",
            "uds are bent\n",
            "To dim his glory and to stain the tra\n",
            "Prediction\n",
            "uds are bentsfo dir his glory and to stain the tr \n",
            "loss: 0.117838  [19264/21870]\n",
            "Input\n",
            "ondon;\n",
            "And many giddy people flock to him.\n",
            "\n",
            "KING H\n",
            "Target\n",
            "ndon;\n",
            "And many giddy people flock to him.\n",
            "\n",
            "KING HE\n",
            "Prediction\n",
            "ndon;\n",
            "End many giddy people flock to hin.\n",
            "\n",
            "KING HA\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.116712  [   64/21870]\n",
            "Input\n",
            "is this? sport?\n",
            "\n",
            "LEONTES:\n",
            "Bear the boy hence; he s\n",
            "Target\n",
            "s this? sport?\n",
            "\n",
            "LEONTES:\n",
            "Bear the boy hence; he sh\n",
            "Prediction\n",
            "s this? sport?\n",
            "\n",
            "\n",
            "EONTES:\n",
            "Bear the boy hence; he st\n",
            "loss: 0.127291  [ 6464/21870]\n",
            "Input\n",
            "oves do peck the falcon's piercing talons;\n",
            "So desp\n",
            "Target\n",
            "ves do peck the falcon's piercing talons;\n",
            "So despe\n",
            "Prediction\n",
            "ves d  peck the faloon's piercing talons;\n",
            "So despe\n",
            "loss: 0.099954  [12864/21870]\n",
            "Input\n",
            " hence, and trouble us not;\n",
            "For thou hast made the\n",
            "Target\n",
            "hence, and trouble us not;\n",
            "For thou hast made the \n",
            "Prediction\n",
            "hence, and trouble us not;\n",
            "For thou hast made the \n",
            "loss: 0.120454  [19264/21870]\n",
            "Input\n",
            "York and Edward wept,\n",
            "To hear the piteous moan tha\n",
            "Target\n",
            "ork and Edward wept,\n",
            "To hear the piteous moan that\n",
            "Prediction\n",
            "ork and Edward wept,\n",
            "To hear the piteous moan t an\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.089709  [   64/21870]\n",
            "Input\n",
            "to be cross in talk,\n",
            "But thou with mildness entert\n",
            "Target\n",
            "o be cross in talk,\n",
            "But thou with mildness enterta\n",
            "Prediction\n",
            "o be cross in talk,\n",
            "But thou with mildness entert \n",
            "loss: 0.157878  [ 6464/21870]\n",
            "Input\n",
            "Of all that hear me, and my near'st of kin\n",
            "Cry fie\n",
            "Target\n",
            "f all that hear me, and my near'st of kin\n",
            "Cry fie \n",
            "Prediction\n",
            "f all that hear me, ano my near'st of kin\n",
            "Cry fie \n",
            "loss: 0.087408  [12864/21870]\n",
            "Input\n",
            ":\n",
            "Look to the baked meats, good Angelica:\n",
            "Spare no\n",
            "Target\n",
            "\n",
            "Look to the baked meats, good Angelica:\n",
            "Spare not\n",
            "Prediction\n",
            "\n",
            "Look to the baked meats, good Angelica:\n",
            "Spare nou\n",
            "loss: 0.094677  [19264/21870]\n",
            "Input\n",
            " Plantagenet\n",
            "Is crown'd so soon, and broke his sol\n",
            "Target\n",
            "Plantagenet\n",
            "Is crown'd so soon, and broke his sole\n",
            "Prediction\n",
            "Puantagenet\n",
            "Is crown'd so soon, and broke his sole\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.061695  [   64/21870]\n",
            "Input\n",
            "me issueless; and your father's blest,\n",
            "As he from \n",
            "Target\n",
            "e issueless; and your father's blest,\n",
            "As he from h\n",
            "Prediction\n",
            "e issueless; and your father's blest,\n",
            "As he from t\n",
            "loss: 0.072817  [ 6464/21870]\n",
            "Input\n",
            "arists of Saint Clare.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Who's th\n",
            "Target\n",
            "rists of Saint Clare.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Who's tha\n",
            "Prediction\n",
            "rists of Saint Clare.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA.\n",
            "Who's the\n",
            "loss: 0.076695  [12864/21870]\n",
            "Input\n",
            "od-den.\n",
            "\n",
            "Nurse:\n",
            "May not one speak?\n",
            "\n",
            "CAPULET:\n",
            "Peace\n",
            "Target\n",
            "d-den.\n",
            "\n",
            "Nurse:\n",
            "May not one speak?\n",
            "\n",
            "CAPULET:\n",
            "Peace,\n",
            "Prediction\n",
            "d-den.\n",
            "\n",
            "Nurse:\n",
            "May not one speak?\n",
            "\n",
            "CAPULET:\n",
            "Peace \n",
            "loss: 0.056504  [19264/21870]\n",
            "Input\n",
            "ith sorrow,\n",
            "Remember Margaret was a prophetess.'\n",
            "C\n",
            "Target\n",
            "th sorrow,\n",
            "Remember Margaret was a prophetess.'\n",
            "Co\n",
            "Prediction\n",
            "th sorrow,\n",
            "Remember Margaret was a prophetess.\n",
            "\n",
            "CH\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.069381  [   64/21870]\n",
            "Input\n",
            "tes--\n",
            "So dry he was for sway--wi' the King of Napl\n",
            "Target\n",
            "es--\n",
            "So dry he was for sway--wi' the King of Naple\n",
            "Prediction\n",
            "es--\n",
            "So dry he was for sway--wi' the King of Napl \n",
            "loss: 0.052844  [ 6464/21870]\n",
            "Input\n",
            "time suppresseth wrongs.\n",
            "\n",
            "WARWICK:\n",
            "Injurious Marga\n",
            "Target\n",
            "ime suppresseth wrongs.\n",
            "\n",
            "WARWICK:\n",
            "Injurious Margar\n",
            "Prediction\n",
            "ime suppresseth wrongs.\n",
            "\n",
            "WARWICK:\n",
            "Injurious Margan\n",
            "loss: 0.065040  [12864/21870]\n",
            "Input\n",
            "h soon-speeding gear\n",
            "As will disperse itself throu\n",
            "Target\n",
            " soon-speeding gear\n",
            "As will disperse itself throug\n",
            "Prediction\n",
            " soon-speeding gear\n",
            "As will disperse itself throul\n",
            "loss: 0.085476  [19264/21870]\n",
            "Input\n",
            "t in all,\n",
            "And usest none in that true use indeed\n",
            "W\n",
            "Target\n",
            " in all,\n",
            "And usest none in that true use indeed\n",
            "Wh\n",
            "Prediction\n",
            " in all,\n",
            "And usest none in that true use indeed\n",
            "Wh\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.061143  [   64/21870]\n",
            "Input\n",
            "IOLANUS:\n",
            "Thou wretch, despite o'erwhelm thee!\n",
            "What\n",
            "Target\n",
            "OLANUS:\n",
            "Thou wretch, despite o'erwhelm thee!\n",
            "What \n",
            "Prediction\n",
            "OLANUS:\n",
            "Thou wretch, despite o'erwhelm thee!\n",
            "What \n",
            "loss: 0.065245  [ 6464/21870]\n",
            "Input\n",
            "IO:\n",
            "Believe me, sir, they butt together well.\n",
            "\n",
            "BIA\n",
            "Target\n",
            "O:\n",
            "Believe me, sir, they butt together well.\n",
            "\n",
            "BIAN\n",
            "Prediction\n",
            "O:\n",
            "Believe me, sir, they butt together well.\n",
            "\n",
            "BIAR\n",
            "loss: 0.058524  [12864/21870]\n",
            "Input\n",
            "s thy lord, thy life, thy keeper,\n",
            "Thy head, thy so\n",
            "Target\n",
            " thy lord, thy life, thy keeper,\n",
            "Thy head, thy sov\n",
            "Prediction\n",
            " thy lord, thy life, thy keeper,\n",
            "Thy head, thy sou\n",
            "loss: 0.099662  [19264/21870]\n",
            "Input\n",
            "rove a tyrant to him. As for you,\n",
            "Say what you can\n",
            "Target\n",
            "ove a tyrant to him. As for you,\n",
            "Say what you can,\n",
            "Prediction\n",
            "ove a tyrant to him. As for you,\n",
            "Say what you cand\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.062243  [   64/21870]\n",
            "Input\n",
            "ar not me.\n",
            "\n",
            "TRANIO:\n",
            "But hast thou done thy errand \n",
            "Target\n",
            "r not me.\n",
            "\n",
            "TRANIO:\n",
            "But hast thou done thy errand t\n",
            "Prediction\n",
            "r not me.\n",
            "\n",
            "TRANIO:\n",
            "But hast thou done thy errand t\n",
            "loss: 0.083484  [ 6464/21870]\n",
            "Input\n",
            "\n",
            "Show duty, as mistaken all this while\n",
            "Between the\n",
            "Target\n",
            "Show duty, as mistaken all this while\n",
            "Between the \n",
            "Prediction\n",
            "Show d ty, as mistaken all this while\n",
            "Between the \n",
            "loss: 0.065633  [12864/21870]\n",
            "Input\n",
            "\n",
            "Wipe off the dust that hides our sceptre's gilt\n",
            "A\n",
            "Target\n",
            "Wipe off the dust that hides our sceptre's gilt\n",
            "An\n",
            "Prediction\n",
            "Wipe tff the dust that hides our sceptre's gilt\n",
            "An\n",
            "loss: 0.059406  [19264/21870]\n",
            "Input\n",
            "een thus trod down,\n",
            "He should have found his uncle\n",
            "Target\n",
            "en thus trod down,\n",
            "He should have found his uncle \n",
            "Prediction\n",
            "en thus trod down,\n",
            "He should have found his uncle \n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.065613  [   64/21870]\n",
            "Input\n",
            " them.\n",
            "\n",
            "COMINIUS:\n",
            "O, you have made good work!\n",
            "\n",
            "MEN\n",
            "Target\n",
            "them.\n",
            "\n",
            "COMINIUS:\n",
            "O, you have made good work!\n",
            "\n",
            "MENE\n",
            "Prediction\n",
            "them.\n",
            "\n",
            "COMINIUS:\n",
            "O, you have made good work!\n",
            "\n",
            "MENC\n",
            "loss: 0.059481  [ 6464/21870]\n",
            "Input\n",
            " in the sentence my own life destroy'd.\n",
            "Alas, I lo\n",
            "Target\n",
            "in the sentence my own life destroy'd.\n",
            "Alas, I loo\n",
            "Prediction\n",
            "in the sentence my own life destroy'd.\n",
            "Alas, I lou\n",
            "loss: 0.055745  [12864/21870]\n",
            "Input\n",
            "ir with the lark to-morrow, gentle Norfolk.\n",
            "\n",
            "NORFO\n",
            "Target\n",
            "r with the lark to-morrow, gentle Norfolk.\n",
            "\n",
            "NORFOL\n",
            "Prediction\n",
            "r with the lark to-morrow, gentle Norfolk.\n",
            "\n",
            "NORFO:\n",
            "loss: 0.066071  [19264/21870]\n",
            "Input\n",
            "beats as it would fall in twenty pieces.\n",
            "My back o\n",
            "Target\n",
            "eats as it would fall in twenty pieces.\n",
            "My back o'\n",
            "Prediction\n",
            "eats as it would fall in twenty pieces.\n",
            "My back ou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model_epoch9.pt\")"
      ],
      "metadata": {
        "id": "GYquG9OhwywG"
      },
      "id": "GYquG9OhwywG",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint['model_state_dict'].keys()"
      ],
      "metadata": {
        "id": "Q-5QU4T61EIK",
        "outputId": "17113905-c73c-49bd-e4f2-886dae80ed96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q-5QU4T61EIK",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['norm.weight', 'norm.bias', 'embedding.embedding.weight', 'embedding.embedding.bias', 'multiheads1.0.WQs.0.weight', 'multiheads1.0.WQs.0.bias', 'multiheads1.0.WQs.1.weight', 'multiheads1.0.WQs.1.bias', 'multiheads1.0.WQs.2.weight', 'multiheads1.0.WQs.2.bias', 'multiheads1.0.WQs.3.weight', 'multiheads1.0.WQs.3.bias', 'multiheads1.0.WQs.4.weight', 'multiheads1.0.WQs.4.bias', 'multiheads1.0.WQs.5.weight', 'multiheads1.0.WQs.5.bias', 'multiheads1.0.WQs.6.weight', 'multiheads1.0.WQs.6.bias', 'multiheads1.0.WQs.7.weight', 'multiheads1.0.WQs.7.bias', 'multiheads1.0.WKs.0.weight', 'multiheads1.0.WKs.0.bias', 'multiheads1.0.WKs.1.weight', 'multiheads1.0.WKs.1.bias', 'multiheads1.0.WKs.2.weight', 'multiheads1.0.WKs.2.bias', 'multiheads1.0.WKs.3.weight', 'multiheads1.0.WKs.3.bias', 'multiheads1.0.WKs.4.weight', 'multiheads1.0.WKs.4.bias', 'multiheads1.0.WKs.5.weight', 'multiheads1.0.WKs.5.bias', 'multiheads1.0.WKs.6.weight', 'multiheads1.0.WKs.6.bias', 'multiheads1.0.WKs.7.weight', 'multiheads1.0.WKs.7.bias', 'multiheads1.0.WVs.0.weight', 'multiheads1.0.WVs.0.bias', 'multiheads1.0.WVs.1.weight', 'multiheads1.0.WVs.1.bias', 'multiheads1.0.WVs.2.weight', 'multiheads1.0.WVs.2.bias', 'multiheads1.0.WVs.3.weight', 'multiheads1.0.WVs.3.bias', 'multiheads1.0.WVs.4.weight', 'multiheads1.0.WVs.4.bias', 'multiheads1.0.WVs.5.weight', 'multiheads1.0.WVs.5.bias', 'multiheads1.0.WVs.6.weight', 'multiheads1.0.WVs.6.bias', 'multiheads1.0.WVs.7.weight', 'multiheads1.0.WVs.7.bias', 'multiheads2.0.WQs.0.weight', 'multiheads2.0.WQs.0.bias', 'multiheads2.0.WQs.1.weight', 'multiheads2.0.WQs.1.bias', 'multiheads2.0.WQs.2.weight', 'multiheads2.0.WQs.2.bias', 'multiheads2.0.WQs.3.weight', 'multiheads2.0.WQs.3.bias', 'multiheads2.0.WQs.4.weight', 'multiheads2.0.WQs.4.bias', 'multiheads2.0.WQs.5.weight', 'multiheads2.0.WQs.5.bias', 'multiheads2.0.WQs.6.weight', 'multiheads2.0.WQs.6.bias', 'multiheads2.0.WQs.7.weight', 'multiheads2.0.WQs.7.bias', 'multiheads2.0.WKs.0.weight', 'multiheads2.0.WKs.0.bias', 'multiheads2.0.WKs.1.weight', 'multiheads2.0.WKs.1.bias', 'multiheads2.0.WKs.2.weight', 'multiheads2.0.WKs.2.bias', 'multiheads2.0.WKs.3.weight', 'multiheads2.0.WKs.3.bias', 'multiheads2.0.WKs.4.weight', 'multiheads2.0.WKs.4.bias', 'multiheads2.0.WKs.5.weight', 'multiheads2.0.WKs.5.bias', 'multiheads2.0.WKs.6.weight', 'multiheads2.0.WKs.6.bias', 'multiheads2.0.WKs.7.weight', 'multiheads2.0.WKs.7.bias', 'multiheads2.0.WVs.0.weight', 'multiheads2.0.WVs.0.bias', 'multiheads2.0.WVs.1.weight', 'multiheads2.0.WVs.1.bias', 'multiheads2.0.WVs.2.weight', 'multiheads2.0.WVs.2.bias', 'multiheads2.0.WVs.3.weight', 'multiheads2.0.WVs.3.bias', 'multiheads2.0.WVs.4.weight', 'multiheads2.0.WVs.4.bias', 'multiheads2.0.WVs.5.weight', 'multiheads2.0.WVs.5.bias', 'multiheads2.0.WVs.6.weight', 'multiheads2.0.WVs.6.bias', 'multiheads2.0.WVs.7.weight', 'multiheads2.0.WVs.7.bias', 'feedforwards.0.0.weight', 'feedforwards.0.0.bias', 'feedforwards.0.2.weight', 'feedforwards.0.2.bias', 'toLogit.weight', 'toLogit.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = LonelyDecoder(model_parameters)\n",
        "mymodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "myoptimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3, betas=(0.9, 0.98))\n",
        "myoptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "print(f\"Loaded checkpoint at epoch {epoch} with loss {loss}\")\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
        "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch\n",
        "    # normalization and dropout layers Unnecessary in this situation\n",
        "    # but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred.transpose(1,2)[:,:,-1], y[:,-1])\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * 64 + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            print(\"Input\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(X[0], dim=1)[1].tolist()]))\n",
        "            print(\"Target\")\n",
        "            print(''.join([idx2char[i] for i in y[0]]))\n",
        "            print(\"Prediction\")\n",
        "            print(''.join([idx2char[i] for i in torch.max(pred[0], dim=1)[1].tolist()]))\n",
        "        torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, f\"model_epoch{t}.pt\")\n",
        "print(\"Train loop\")\n",
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer, t)\n",
        "run_n_epochs(1, train_dataloader, mymodel, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "MTk8uLaRw7tA",
        "outputId": "80cd89fe-c29c-4859-bcf5-2e9670af8d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        }
      },
      "id": "MTk8uLaRw7tA",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint at epoch 10 with loss 0.08523036539554596\n",
            "Train loop\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.264168  [   64/21870]\n",
            "Input\n",
            " at me, make their pastime at my sorrow:\n",
            "They shou\n",
            "Target\n",
            "at me, make their pastime at my sorrow:\n",
            "They shoul\n",
            "Prediction\n",
            "at me, make their pastime at my soreow:\n",
            "They shous\n",
            "loss: 2.465627  [ 6464/21870]\n",
            "Input\n",
            "t want their remedies.\n",
            "Cousin, I am too young to b\n",
            "Target\n",
            " want their remedies.\n",
            "Cousin, I am too young to be\n",
            "Prediction\n",
            " want their remedies.\n",
            "Cousin, I am too young to be\n",
            "loss: 2.635937  [12864/21870]\n",
            "Input\n",
            "h to me to be at enmity;\n",
            "I hate it, and desire all\n",
            "Target\n",
            " to me to be at enmity;\n",
            "I hate it, and desire all \n",
            "Prediction\n",
            " to me to be at enmity;\n",
            "I hate it, and desire alle\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36mrun_n_epochs\u001b[0;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-99a323ce3c56>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, t)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "decoder.eval()\n",
        "print(''.join([idx2char[i] for i in torch.max(x[30], dim=1)[1].tolist()]))\n",
        "print(''.join([idx2char[i] for i in torch.max(decoder(x[30].squeeze(0))[0],1)[1].tolist()]))\n",
        "initString = \"Of civil wounds plough'd up with neighbours' sword,\"\n",
        "examples = [char2idx[c] for c in initString]\n",
        "one_hot_initString = F.one_hot(torch.tensor(examples[:-1]).long(),\n",
        "                             model_parameters[\"vocabulary_size\"]).float()\n",
        "print(one_hot_initString.shape)\n",
        "print(\"###\")\n",
        "decoder.eval()\n",
        "print(''.join([idx2char[i] for i in torch.max(decoder(one_hot_initString.squeeze(0))[0],1)[1].tolist()]))\n",
        "print(\"###\")"
      ],
      "metadata": {
        "id": "fXi_qqUdnmko",
        "outputId": "b638fdc5-7610-430f-b31f-8555d94846e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fXi_qqUdnmko",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 50, 67])\n",
            "g the queen thereof.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "A little joy\n",
            "                                                  \n",
            "torch.Size([50, 67])\n",
            "###\n",
            "                                                  \n",
            "###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder)"
      ],
      "metadata": {
        "id": "bzA5ndfgKu3O",
        "outputId": "2f12130e-25cf-446e-a87c-aeb9988df2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bzA5ndfgKu3O",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LonelyDecoder(\n",
            "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (embedding): Embedding(\n",
            "    (embedding): Linear(in_features=67, out_features=256, bias=True)\n",
            "  )\n",
            "  (multiheads1): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (multiheads2): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (WQs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WKs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (WVs): ModuleList(\n",
            "        (0-7): 8 x Linear(in_features=256, out_features=32, bias=True)\n",
            "      )\n",
            "      (spda): ScaledDotProductAttention(\n",
            "        (softmax): Softmax(dim=None)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feedforwards): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (toLogit): Linear(in_features=256, out_features=67, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "x,y = next(iter(train_dataloader))\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(torch.max(x[0], dim=1)[1].tolist())\n",
        "print(y[0])"
      ],
      "metadata": {
        "id": "9tSdscqGACDC",
        "outputId": "e2db63e9-f08d-4c46-8831-dca34ce4643c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9tSdscqGACDC",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 8, 67])\n",
            "torch.Size([64, 8])\n",
            "[20, 49, 58, 59, 60, 1, 17, 49]\n",
            "tensor([49, 58, 59, 60,  1, 17, 49, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "f5a15921-6375-4b45-89ab-63391172ce28",
      "metadata": {
        "id": "f5a15921-6375-4b45-89ab-63391172ce28",
        "outputId": "ce2d8939-861a-46f0-8b26-071127d0886e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 4])\n",
            "tensor([[0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "tensor(0.9048)\n"
          ]
        }
      ],
      "source": [
        "# Example of target with class indices\n",
        "loss = nn.CrossEntropyLoss()\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "input = F.one_hot(target, num_classes=5).float()\n",
        "print(target)\n",
        "print(input)\n",
        "output = loss(input, target)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_n_epochs(epochs, dataloader, model, loss_fn, optimizer):\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {5+t+1}\\n-------------------------------\")\n",
        "        train_loop(dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "run_n_epochs(45, train_dataloader, decoder, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "TLq7ZWnHr104",
        "outputId": "98f9b8bb-56a2-44ae-9160-82b58e63a5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "TLq7ZWnHr104",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 4.009966  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XHakevm'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 3.983451  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "qZ mfjj,\n",
            "loss: 3.990031  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "obQQfQzZ\n",
            "loss: 4.036373  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "whWQooX \n",
            "loss: 4.001715  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-gQGakn,\n",
            "loss: 4.010240  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "g,#.TTJy\n",
            "loss: 4.002563  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "o-r.merx\n",
            "loss: 4.007478  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "our\n",
            "Miou\n",
            "loss: 3.987219  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "asbib!bj\n",
            "loss: 4.015161  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XuyxVkuc\n",
            "loss: 4.015061  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "yd,\n",
            "wiad\n",
            "loss: 4.015932  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "T&eqtqzt\n",
            "loss: 3.983913  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "Wha.Wr'\n",
            "loss: 4.002244  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "nd;-n-;P\n",
            "loss: 4.000215  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "';'Pjjzn\n",
            "loss: 4.019834  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "b;exQ;Q\n",
            "loss: 3.983692  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q KavevH\n",
            "loss: 4.007519  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q3ousJ;@\n",
            "loss: 4.042298  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            " t,3I Ih\n",
            "loss: 3.993047  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i3-his$q\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 4.009781  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "'D'JI.f.\n",
            "loss: 4.008904  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "haoj3unc\n",
            "loss: 4.020438  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "rrQYyQro\n",
            "loss: 3.994052  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "33;iin z\n",
            "loss: 4.010096  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jxjQjj-j\n",
            "loss: 3.965109  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "33myx c-\n",
            "loss: 3.986863  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "s'zTXs''\n",
            "loss: 3.987497  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "'Mh''Wj'\n",
            "loss: 3.996432  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "SJvX-@xZ\n",
            "loss: 4.019480  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XQQrg&xn\n",
            "loss: 4.010562  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Bikxq;bn\n",
            "loss: 4.002163  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "d3??tzt-\n",
            "loss: 3.961648  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jxjz;bfJ\n",
            "loss: 3.993537  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ".f'TfofJ\n",
            "loss: 3.964676  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "jsXqss:\n",
            "\n",
            "loss: 3.987367  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "!n3al!'J\n",
            "loss: 4.018489  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "bexTQ-?r\n",
            "loss: 3.991456  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "PrH FFrJ\n",
            "loss: 4.018596  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "!wJQzo! \n",
            "loss: 4.004164  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ZxxZdZpr\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 3.998062  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "3KikkQkk\n",
            "loss: 4.010931  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "q3h&goug\n",
            "loss: 3.999441  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "uX!uzO:l\n",
            "loss: 3.995297  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ".zzzzzz.\n",
            "loss: 3.964694  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ankakBin\n",
            "loss: 4.009676  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i:wizh3w\n",
            "loss: 3.977067  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ikkk! ki\n",
            "loss: 3.997163  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "zmyZWiZt\n",
            "loss: 3.982253  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "OPLT'OY:\n",
            "loss: 3.984232  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-.gj3-g3\n",
            "loss: 4.019158  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ":&ffoD?n\n",
            "loss: 3.983885  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "J3ul!!Zd\n",
            "loss: 3.981462  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "k?qcc?c.\n",
            "loss: 4.005831  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "wiz!iu?-\n",
            "loss: 3.984880  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "mvev;eqv\n",
            "loss: 3.976781  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "emmk m,k\n",
            "loss: 4.008554  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "bppXpk, \n",
            "loss: 3.977186  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "XttXt!?w\n",
            "loss: 3.971331  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "\n",
            "CAARCHC\n",
            "loss: 3.974417  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "v?,X!vz!\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 3.993505  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "j'Poxz3c\n",
            "loss: 3.992613  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Ubjavxav\n",
            "loss: 4.007241  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "thq3itt?\n",
            "loss: 4.004530  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-bb?bzZb\n",
            "loss: 3.995043  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "i qsqisi\n",
            "loss: 4.002447  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "I:ICVI\n",
            "I\n",
            "loss: 3.974081  [38464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "3cqck3cl\n",
            "loss: 3.973033  [44864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ET\n",
            "Why,\n",
            "\n",
            "loss: 3.994256  [51264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "qD33Q3ex\n",
            "loss: 3.968574  [57664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "Hbbzzz;M\n",
            "loss: 4.008118  [64064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "?cq??x?\n",
            "\n",
            "loss: 3.995346  [70464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "xss;s:Z@\n",
            "loss: 3.988353  [76864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "ZqbyqPeq\n",
            "loss: 3.990261  [83264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "pcXpVXWh\n",
            "loss: 3.999664  [89664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "JQv!x$rq\n",
            "loss: 3.981891  [96064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "xvJuofuZ\n",
            "loss: 3.998492  [102464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "j;zbhexK\n",
            "loss: 4.008811  [108864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "-zirQZ!W\n",
            "loss: 4.020496  [115264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "l??J??-W\n",
            "loss: 3.970493  [121664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "l3q!avll\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 3.997797  [   64/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "GRCDCHAG\n",
            "loss: 3.980371  [ 6464/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "r-ZGufZJ\n",
            "loss: 4.005164  [12864/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "OOLXL:qw\n",
            "loss: 3.998323  [19264/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            ",xQ, c,;\n",
            "loss: 3.976596  [25664/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "vvvDUsev\n",
            "loss: 3.989638  [32064/123932]\n",
            "Target\n",
            "irst Cit\n",
            "Prediction\n",
            "HxYousyZ\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-291c39be0d3b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-291c39be0d3b>\u001b[0m in \u001b[0;36mrun_n_epochs\u001b[0;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {5+t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrun_n_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-38241210b6fc>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W51VPcfq2V76"
      },
      "id": "W51VPcfq2V76",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
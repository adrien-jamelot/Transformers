My short term goal is to reproduce a practical I did 4-5 years ago at uni where we used an LSTM trained on Shakespeare's texts to generate more Shakespeare-like tests. This practical was very probably inspired from Karpathy's famous blogpost on [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). Since then, the Deep Learning world has moved to Transformers and I want to understand more in depth what makes them different. To spice up the challenge, I'm using as little external resources as possible and try to keep myself to the research paper itself and pytorch API's doc.

In the summer 2024, I've worked out a functional training pipeline for the decoder and left the project there. Now it's summer 2025, and I am focusing on cleaning and packaging the repo. My objectives are

- [] Re-implementing the working ipynb notebook as a coherent set of python files
- [] Cleaning up my notes
- [] Ensuring coding best practices
- [] Integrating mlflow
- [] Packaging, containerising, facilitating shipping to cloud platforms

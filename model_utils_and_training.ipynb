{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49b63e5-0c97-470b-817e-0f82f14dfd8d",
   "metadata": {},
   "source": [
    "# Utils and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72566f-8da4-40a0-851d-54fe28fb7636",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcd3cb-50f6-4a83-9113-15a6f1f2ba28",
   "metadata": {},
   "source": [
    "### addAndNorm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6ea7fd-ec7d-4c37-b5e8-73e663b1ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Residual connection.\"\"\"\n",
    "\n",
    "\n",
    "def addAndNorm(x, blockOutput, norm):\n",
    "    \"\"\"Residual connection.\"\"\"\n",
    "    return norm(x + blockOutput)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176556f-9240-493a-93d9-8f3a0c9b42ef",
   "metadata": {},
   "source": [
    "### PositionalEncoding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9923029-9788-4ce9-8518-2b21105d3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Positional Encoding.\"\"\"\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def positionalEncoding(x, dim_model):\n",
    "    \"\"\"Positional Encoding.\"\"\"\n",
    "    def sineOrCosine(i):\n",
    "        \"\"\"sin(alpha+pi/2) = cos(alpha).\"\"\"\n",
    "        return math.pi/2*(i % 2 == 1)\n",
    "    values = [\n",
    "        [sineOrCosine(i) + pos/math.pow(10000, 2*(i//2)/dim_model) for\n",
    "         i in range(dim_model)]\n",
    "        for pos in range(x.shape[0])\n",
    "    ]\n",
    "    return torch.sin(torch.tensor(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106be76-87e9-405c-a431-8569ef02043a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9297b179-d38c-4cb9-a023-4f32a6d26367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Main transformer block.\n",
    "\n",
    "    Inputs: - model_parameters: ordered dictionary of key-values describing the\n",
    "    layer parameters of the model:\n",
    "      - dim_model: dimension of the model.\n",
    "      - layers: dictionary of key-values describing specific layers\n",
    "        - <layer_name>: dictionary of parameters for the specific multi-head\n",
    "          layer\n",
    "          - attention: dictionary of parameters for the specific attention\n",
    "            function\n",
    "            - dim_key: dimension of the key and query.\n",
    "            - dim_value: dimension of the value.\n",
    "          - nb_head: number of heads.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_parameters):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_model = model_parameters[\"dim_model\"]\n",
    "        self.encoder = Encoder(model_parameters[\"encoder\"])\n",
    "        self.decoder = Decoder(model_parameters[\"decoder\"])\n",
    "        self.embedding = Embedding(model_parameters)\n",
    "        self.toProba = nn.Sequential(\n",
    "            nn.Linear(self.dim_model,\n",
    "                      model_parameters[\"vocabulary_size\"]),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, lastOutput):\n",
    "        \"\"\"Apply a step forward.\"\"\"\n",
    "        encoderInput = self.embedding(x) + positionalEncoding(\n",
    "            x, self.dim_model)\n",
    "        decoderInput = self.embedding(lastOutput) + positionalEncoding(\n",
    "            lastOutput, self.dim_model)\n",
    "        encoderInput = self.dropout(encoderInput)\n",
    "        decoderInput = self.dropout(decoderInput)\n",
    "        encoderOutput = self.encoder(encoderInput)\n",
    "        decoderOutput = self.decoder(decoderInput, encoderOutput)\n",
    "        lastOutput = self.toProba(decoderOutput)\n",
    "        return lastOutput\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, encoderConfig):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "        self.nb_layers = encoderConfig[\"nb_layers\"]\n",
    "        self.dim_model = encoderConfig[\"dim_model\"]\n",
    "        self.dim_feedforward = encoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
    "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model,\n",
    "                                 elementwise_affine=True, bias=True)\n",
    "        self.multiheads = [MultiHeadAttention(encoderConfig[\"multihead\"],\n",
    "                                              masked=False)\n",
    "                           for i in range(self.nb_layers)]\n",
    "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
    "                                                     self.dim_feedforward),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(self.dim_feedforward,\n",
    "                                                     self.dim_model))\n",
    "                             for i in range(self.nb_layers)]\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        for i in range(self.nb_layers):\n",
    "            h1 = addAndNorm(x, self.dropout(self.multiheads[i](x, x,\n",
    "                                                               x)),\n",
    "                            self.norm)\n",
    "            x = addAndNorm(h1, self.dropout(self.feedforwards[i](h1)),\n",
    "                           self.norm)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, decoderConfig):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_model = decoderConfig[\"dim_model\"]\n",
    "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
    "        self.dim_feedforward = decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
    "        self.nb_layers = decoderConfig[\"nb_layers\"]\n",
    "        self.layer = []\n",
    "        self.multiheads1 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
    "                                               masked=True)\n",
    "                            for i in range(self.nb_layers)]\n",
    "        self.multiheads2 = [MultiHeadAttention(decoderConfig[\"multihead\"],\n",
    "                                               masked=False)\n",
    "                            for i in range(self.nb_layers)]\n",
    "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
    "                                                     self.dim_feedforward),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(self.dim_feedforward,\n",
    "                                                     self.dim_model))\n",
    "                             for i in range(self.nb_layers)]\n",
    "\n",
    "    def forward(self, decoderInput, encoderOutput):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        for i in range(self.nb_layers):\n",
    "            h1 = addAndNorm(decoderInput,\n",
    "                            self.multiheads1[i](decoderInput,\n",
    "                                                decoderInput,\n",
    "                                                decoderInput),\n",
    "                            self.norm)\n",
    "            h2 = addAndNorm(h1,\n",
    "                            self.multiheads2[i](h1,\n",
    "                                                encoderOutput,\n",
    "                                                encoderOutput),\n",
    "                            self.norm)\n",
    "            lastOutput = addAndNorm(h2,\n",
    "                                    self.feedforwards[i](h2),\n",
    "                                    self.norm)\n",
    "        return lastOutput\n",
    "\n",
    "\n",
    "class LonelyDecoder(nn.Module):\n",
    "    \"\"\"A lonely decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, model_parameters):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "        self.decoderConfig = model_parameters[\"decoder\"]\n",
    "        self.dim_model = self.decoderConfig[\"dim_model\"]\n",
    "        self.norm = nn.LayerNorm(normalized_shape=self.dim_model)\n",
    "        self.dim_feedforward = self.decoderConfig[\"feedforward\"][\"dim_feedforward\"]\n",
    "        self.nb_layers = self.decoderConfig[\"nb_layers\"]\n",
    "        self.layer = []\n",
    "        self.embedding = Embedding(model_parameters)\n",
    "        self.multiheads1 = [MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
    "                                               masked=True)\n",
    "                            for i in range(self.nb_layers)]\n",
    "        self.multiheads2 = [MultiHeadAttention(self.decoderConfig[\"multihead\"],\n",
    "                                               masked=False)\n",
    "                            for i in range(self.nb_layers)]\n",
    "        self.feedforwards = [nn.Sequential(nn.Linear(self.dim_model,\n",
    "                                                     self.dim_feedforward),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(self.dim_feedforward,\n",
    "                                                     self.dim_model))\n",
    "                             for i in range(self.nb_layers)]\n",
    "        self.toProba = nn.Sequential(\n",
    "            nn.Linear(self.dim_model,\n",
    "                      model_parameters[\"vocabulary_size\"]),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        x = self.embedding(x) + positionalEncoding(\n",
    "            x, self.dim_model)\n",
    "        for i in range(self.nb_layers):\n",
    "            h1 = addAndNorm(x,\n",
    "                            self.multiheads1[i](x, x, x),\n",
    "                            self.norm)\n",
    "            h2 = addAndNorm(h1,\n",
    "                            self.multiheads2[i](h1, h1, h1),\n",
    "                            self.norm)\n",
    "            layerOutput = addAndNorm(h2,\n",
    "                                     self.feedforwards[i](h2),\n",
    "                                     self.norm)\n",
    "        finalOutput = self.toProba(layerOutput)\n",
    "        return finalOutput\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, masked=False):\n",
    "        \"\"\"Initialize.\n",
    "\n",
    "        Inputs:\n",
    "        - dim_model: model dimension\n",
    "        - masked: prevents tokens to attend to the following ones.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.masked = masked\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Inputs:\n",
    "        - Q: query\n",
    "        - K: key\n",
    "        - V: value\n",
    "        \"\"\"\n",
    "        matmul_0 = torch.matmul(Q, K.transpose(0, 1))\n",
    "        scaled = torch.divide(matmul_0, torch.Tensor([self.dim_model]))\n",
    "        if self.masked:\n",
    "            mask = torch.ones(scaled.shape)\n",
    "            mask = mask - torch.tril(mask)*mask\n",
    "            mask = torch.where(mask == 1, float('-inf'), 0)\n",
    "            scaled = scaled + mask\n",
    "        softmaxed = self.softmax(scaled)\n",
    "        sdpa = torch.matmul(softmaxed, V)\n",
    "        return sdpa\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention.\n",
    "\n",
    "    Inputs:\n",
    "    - multi_head_config: dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, multi_head_config, masked=False):\n",
    "        \"\"\"Initialize multi-head.\"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_key = multi_head_config[\"attention\"][\"dim_key\"]\n",
    "        self.dim_value = multi_head_config[\"attention\"][\"dim_value\"]\n",
    "        self.nb_heads = multi_head_config[\"nb_heads\"]\n",
    "        self.dim_model = self.dim_key * self.nb_heads\n",
    "\n",
    "        self.WQs = [nn.Linear(self.dim_model, self.dim_key)\n",
    "                    for i in range(self.nb_heads)]\n",
    "        self.WKs = [nn.Linear(self.dim_model, self.dim_key)\n",
    "                    for i in range(self.nb_heads)]\n",
    "        self.WVs = [nn.Linear(self.dim_model, self.dim_value)\n",
    "                    for i in range(self.nb_heads)]\n",
    "        self.spda = ScaledDotProductAttention(self.dim_model, masked)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \"\"\"One step of the multi-head block.\"\"\"\n",
    "        heads = [self.spda(self.WQs[i](Q),\n",
    "                           self.WKs[i](K),\n",
    "                           self.WVs[i](V))\n",
    "                 for i in range(self.nb_heads)]\n",
    "        return torch.cat([head for head in heads], 1)\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, model_parameters):\n",
    "        \"\"\"Initialize embedding.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(model_parameters[\"vocabulary_size\"],\n",
    "                                   model_parameters[\"dim_model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step.\"\"\"\n",
    "        return self.embedding(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d82e56-e9f2-4551-9461-6a16c6824a42",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199362f1-377e-4955-86c2-f775b520b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_shakespeare = open(\"\"\"..\\\\Datasets\\\\tiny_shakespeare.txt\"\"\",\n",
    "            'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4af4e-d6f7-4380-b6dd-bc270c76644d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae36615-9abe-48c3-851b-e1d88cc4ed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "67 unique characters\n",
      "char2idx\n",
      "{'\\n': 0, ' ': 1, '!': 2, '#': 3, '$': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '3': 10, ':': 11, ';': 12, '?': 13, '@': 14, 'A': 15, 'B': 16, 'C': 17, 'D': 18, 'E': 19, 'F': 20, 'G': 21, 'H': 22, 'I': 23, 'J': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'O': 29, 'P': 30, 'Q': 31, 'R': 32, 'S': 33, 'T': 34, 'U': 35, 'V': 36, 'W': 37, 'X': 38, 'Y': 39, 'Z': 40, 'a': 41, 'b': 42, 'c': 43, 'd': 44, 'e': 45, 'f': 46, 'g': 47, 'h': 48, 'i': 49, 'j': 50, 'k': 51, 'l': 52, 'm': 53, 'n': 54, 'o': 55, 'p': 56, 'q': 57, 'r': 58, 's': 59, 't': 60, 'u': 61, 'v': 62, 'w': 63, 'x': 64, 'y': 65, 'z': 66}\n",
      "idx2char\n",
      "['\\n' ' ' '!' '#' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' '@' 'A' 'B' 'C'\n",
      " 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U'\n",
      " 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm'\n",
      " 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "text_as_int\n",
      "[20 49 58 ... 47  9  0]\n",
      "'First Citizen' ---- characters mapped to int ---- >[20 49 58 59 60  1 17 49 60 49 66 45 54]\n",
      "tensor([20, 49, 58,  ..., 47,  9,  0], dtype=torch.int32)\n",
      "There are 8647 chunks of 128 characters available for the\n",
      "network training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Training.\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# # import Transformers.model.transformer.Transformer\n",
    "\n",
    "text = tiny_shakespeare\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])\n",
    "\n",
    "# unique characters in the file\n",
    "vocab = sorted(set(text+\"@\"+\"#\")) # @ will be the initial character\n",
    "                                  # and # the final character. They\n",
    "                                  # are not in the text.\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Lookup tables\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(\"char2idx\")\n",
    "print(char2idx)\n",
    "print(\"idx2char\")\n",
    "print(idx2char)\n",
    "print(\"text_as_int\")\n",
    "print(text_as_int)\n",
    "print ('{} ---- characters mapped to int ---- >{}'.format(repr(text[:13]), text_as_int[:13]))\n",
    "\n",
    "# Create training examples:\n",
    "seq_length = 128\n",
    "examples_per_epoch = len(text)//(seq_length)\n",
    "\n",
    "int_text_tensor = torch.tensor(text_as_int)\n",
    "chunks = torch.chunk(int_text_tensor, examples_per_epoch, 0)\n",
    "print(int_text_tensor)\n",
    "\n",
    "examples = [chunk[:-1] for chunk in chunks]\n",
    "targets = [chunk[1:] for chunk in chunks]\n",
    "print(f\"\"\"There are {len(examples)} chunks of 128 characters available for the\n",
    "network training.\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "model_parameters = {\n",
    "    \"dim_model\": 256,\n",
    "    \"vocabulary_size\": 67,\n",
    "    \"batch_size\": 64,\n",
    "    \"encoder\": {\n",
    "        \"nb_layers\": 1,\n",
    "        \"dim_model\": 256,\n",
    "        \"multihead\": {\n",
    "            \"attention\": {\n",
    "                \"dim_model\": 256,\n",
    "                \"dim_key\": 128,\n",
    "                \"dim_value\": 128\n",
    "                },\n",
    "            \"nb_heads\": 2\n",
    "            },\n",
    "        \"feedforward\": {\n",
    "            \"dim_feedforward\": 256\n",
    "            }\n",
    "        },\n",
    "    \"decoder\": {\n",
    "        \"nb_layers\": 1,\n",
    "        \"vocabulary_size\": 67,\n",
    "        \"dim_model\": 256,\n",
    "        \"multihead\": {\n",
    "            \"attention\": {\n",
    "                \"dim_model\": 256,\n",
    "                \"dim_key\": 128,\n",
    "                \"dim_value\": 128,\n",
    "            },\n",
    "            \"nb_heads\": 2\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"dim_feedforward\": 256\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# transformer = Transformer(model_parameters)\n",
    "# x = torch.randn(10, 128)\n",
    "# lastOutput = torch.randn(3, 128)\n",
    "# transformer(x, lastOutput)\n",
    "\n",
    "decoder = LonelyDecoder(model_parameters)\n",
    "x = torch.randn(10, model_parameters[\"vocabulary_size\"])\n",
    "lastOutput = torch.randn(3, model_parameters[\"vocabulary_size\"])\n",
    "decoder(x)\n",
    "\n",
    "\n",
    "one_hot_examples = F.one_hot(torch.stack(examples[:-1]).long(),\n",
    "                             model_parameters[\"vocabulary_size\"]).float()\n",
    "one_hot_targets = F.one_hot(torch.stack(targets[:-1]).long(), model_parameters[\"vocabulary_size\"]).float()\n",
    "\n",
    "decoder(one_hot_examples[0])\n",
    "one_hot_examples[0]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(decoder(one_hot_examples[0]), one_hot_targets[0])\n",
    "loss.backward()\n",
    "optimizer = torch.optim.SGD(decoder.parameters())\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "data = torch.stack((one_hot_examples, one_hot_targets), dim=0)\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data  = data\n",
    "    def __len__(self):\n",
    "        return data.shape[1]\n",
    "    def __getitem__(self, idx):\n",
    "        return data[0,idx], data[1, idx]\n",
    "\n",
    "dataset = customDataset(data)\n",
    "\n",
    "train_dataloader = DataLoader(customDataset(data[:,0:10]), batch_size=1, shuffle=True)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"Train loop. Taken from pytorch tutorial.\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch\n",
    "    # normalization and dropout layers Unnecessary in this situation\n",
    "    # but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = char2idx[\"@\"]\n",
    "        pred = F.one_hot(torch.Tensor([pred]).long(),\n",
    "                         model_parameters[\"vocabulary_size\"]).float()\n",
    "        print(f\"Initial shape: {pred.shape}\")\n",
    "        for i in range(seq_length-1):\n",
    "            print(f\"Shape of the prediction: {model(pred).shape}\")\n",
    "            pred = torch.cat((pred, model(pred)))\n",
    "            print(f\"Shape at iteration {i}: {pred.shape}\")\n",
    "            print(\"Prediction:\")\n",
    "            print(''.join([idx2char[i] for i in torch.max(pred,\n",
    "                                                          1)[1].tolist()]))\n",
    "        loss = loss_fn(pred[1:], y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# train_loop(train_dataloader, decoder, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "# print(f\"Using {device} device\")\n",
    "# #print(transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a15921-6375-4b45-89ab-63391172ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
